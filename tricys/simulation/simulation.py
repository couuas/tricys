import argparse
import concurrent.futures
import glob
import importlib
import importlib.util
import logging
import os
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
from OMPython import ModelicaSystem

from tricys.core.interceptor import integrate_interceptor_model
from tricys.core.jobs import generate_simulation_jobs
from tricys.core.modelica import (
    format_parameter_value,
    get_om_session,
    load_modelica_package,
)
from tricys.utils.config_utils import basic_prepare_config
from tricys.utils.file_utils import get_unique_filename
from tricys.utils.log_utils import setup_logging

# Standard logger setup
logger = logging.getLogger(__name__)


def _build_co_sim_templates(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    协同仿真预编译阶段 (Compile Once 策略) - 修正版。
    包含:
    1. 强力清理逻辑 (防止中间文件冲突)。
    2. 优先从 Config 读取 output_placeholder (解决 Stage 2 编译缺少列映射的问题)。
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]
    co_sim_config = config["co_simulation"]

    # 1. 准备构建目录
    temp_dir = os.path.abspath(paths_config.get("temp_dir", "temp"))
    build_dir = os.path.join(temp_dir, "co_sim_build_master")

    if os.path.exists(build_dir):
        shutil.rmtree(build_dir)
    os.makedirs(build_dir, exist_ok=True)

    logger.info(f"Phase 1: Building Co-Simulation Templates in {build_dir}...")

    # 2. 复制模型包
    original_package_path = os.path.abspath(paths_config["package_path"])
    isolated_package_path = ""

    if os.path.isfile(original_package_path) and not original_package_path.endswith(
        "package.mo"
    ):
        dest_path = os.path.join(build_dir, os.path.basename(original_package_path))
        shutil.copy(original_package_path, dest_path)
        isolated_package_path = dest_path
    else:
        if os.path.isfile(original_package_path):
            src_dir = os.path.dirname(original_package_path)
        else:
            src_dir = original_package_path

        pkg_folder_name = os.path.basename(src_dir)
        dest_dir = os.path.join(build_dir, pkg_folder_name)

        shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)

        if os.path.exists(os.path.join(dest_dir, "package.mo")):
            isolated_package_path = os.path.join(dest_dir, "package.mo")
        else:
            isolated_package_path = os.path.join(dest_dir, f"{pkg_folder_name}.mo")

    # === 内部辅助函数：归档与清理 ===
    def _archive_and_clean_artifacts(
        base_model_name: str, stage_prefix: str
    ) -> Dict[str, str]:
        exe_ext = ".exe" if os.name == "nt" else ""
        candidates = glob.glob(os.path.join(build_dir, f"{base_model_name}*"))
        archived_paths = {"exe": "", "xml": ""}
        garbage_exts = {".c", ".h", ".o", ".cpp", ".log", ".makefile", ".libs", ".json"}

        for src_path in candidates:
            if not os.path.exists(src_path):
                continue
            filename = os.path.basename(src_path)
            _, ext = os.path.splitext(filename)

            if ext.lower() == ".mo":
                continue  # 保护源文件

            is_artifact = (
                filename == f"{base_model_name}{exe_ext}"
                or filename.endswith(".xml")
                or filename.endswith(".bin")
            )

            if is_artifact:
                new_filename = f"{stage_prefix}_{filename}"
                dst_path = os.path.join(build_dir, new_filename)
                try:
                    shutil.move(src_path, dst_path)
                    if filename == f"{base_model_name}{exe_ext}":
                        archived_paths["exe"] = dst_path
                    elif filename == f"{base_model_name}_init.xml":
                        archived_paths["xml"] = dst_path
                except OSError as e:
                    logger.warning(f"Failed to archive {filename}: {e}")
            elif ext.lower() in garbage_exts or filename == "Makefile":
                try:
                    if os.path.isdir(src_path):
                        shutil.rmtree(src_path)
                    else:
                        os.remove(src_path)
                except OSError:
                    pass

        if not archived_paths["exe"]:
            raise RuntimeError(f"Failed to archive EXE for {stage_prefix}.")
        return archived_paths

    omc = get_om_session()
    try:
        omc.sendExpression(f'cd("{Path(build_dir).as_posix()}")')

        # ======================================================================
        # Step A: 编译 Stage 1 (Original)
        # ======================================================================
        logger.info("Building Stage 1 (Original) Model...")
        if not load_modelica_package(omc, Path(isolated_package_path).as_posix()):
            raise RuntimeError("Failed to load package for Stage 1 build.")

        model_name = sim_config["model_name"]
        if not omc.sendExpression(f"buildModel({model_name})"):
            raise RuntimeError(
                f"Stage 1 build failed: {omc.sendExpression('getErrorString()')}"
            )

        stage1_artifacts = _archive_and_clean_artifacts(model_name, "Stage1")
        logger.info("Stage 1 archived.")

        # ======================================================================
        # Step B: 准备拦截配置 (核心修改部分)
        # ======================================================================
        handlers = co_sim_config.get("handlers", [])
        if not isinstance(handlers, list):
            handlers = [handlers]

        generic_interception_configs = []
        csv_mapping = {}

        for handler in handlers:
            instance_name = handler["instance_name"]
            submodel_name = handler["submodel_name"]
            generic_csv_name = f"{instance_name}_buffer.csv"
            csv_mapping[instance_name] = generic_csv_name

            output_placeholder = {}

            # === 修改开始: 优先使用 Config 中的 output_placeholder ===
            if "output_placeholder" in handler:
                # 1. 优先策略：用户显式配置
                # 格式可能是字典 {"port": "{1,2}"} 或直接是对象，取决于 config 解析方式
                # 这里假设已经是解析好的 Python 对象 (list/str)
                output_placeholder = handler["output_placeholder"]
                logger.debug(
                    f"Using explicitly configured output_placeholder for {instance_name}"
                )
            else:
                # 2. 回退策略：自动探测 (Fallback)
                logger.info(
                    f"Auto-detecting ports for {submodel_name} (No explicit placeholder found)..."
                )
                components = omc.sendExpression(f"getComponents({submodel_name})")
                current_col_idx = 2
                for comp in components:
                    if comp[0] == "Modelica.Blocks.Interfaces.RealOutput":
                        port_name = comp[1]
                        dims = comp[11]
                        dim = int(dims[0]) if dims and dims[0] != "" else 1
                        # 生成列索引列表 [2, 3, 4]
                        output_placeholder[port_name] = list(
                            range(current_col_idx, current_col_idx + dim)
                        )
                        current_col_idx += dim
            # === 修改结束 ===

            config_copy = handler.copy()
            config_copy["csv_uri"] = generic_csv_name
            config_copy["mode"] = co_sim_config.get("mode", "interceptor")

            # 强制更新/填入 placeholder
            config_copy["output_placeholder"] = output_placeholder

            generic_interception_configs.append(config_copy)

        # ======================================================================
        # Step C: 编译 Stage 2 (Intercepted)
        # ======================================================================
        logger.info("Modifying Model structure for Stage 2...")
        mod_result = integrate_interceptor_model(
            package_path=isolated_package_path,
            model_name=model_name,
            interception_configs=generic_interception_configs,
        )

        logger.info("Building Stage 2 (Intercepted) Model...")
        final_model_name = f"{model_name}_Intercepted"

        omc.sendExpression("clear()")

        if co_sim_config.get("mode") == "replacement":
            final_model_name = model_name
        else:

            for model_path in mod_result["interceptor_model_paths"]:
                omc.sendExpression(f"""loadFile("{Path(model_path).as_posix()}")""")
            omc.sendExpression(
                f"""loadFile("{Path(mod_result["system_model_path"]).as_posix()}")"""
            )

        if not load_modelica_package(omc, Path(isolated_package_path).as_posix()):
            sys_path = mod_result.get("system_model_path", isolated_package_path)
            if not load_modelica_package(omc, Path(sys_path).as_posix()):
                raise RuntimeError("Failed to reload package after interception.")

        if not omc.sendExpression(f"buildModel({final_model_name})"):
            raise RuntimeError(
                f"Stage 2 build failed: {omc.sendExpression('getErrorString()')}"
            )

        stage2_artifacts = _archive_and_clean_artifacts(final_model_name, "Stage2")
        logger.info("Stage 2 archived.")

        om_home = omc.sendExpression("getInstallationDirectoryPath()")
        om_bin_path = os.path.join(om_home, "bin")

        return {
            "stage1": stage1_artifacts,
            "stage2": stage2_artifacts,
            "om_bin": om_bin_path,
            "csv_mapping": csv_mapping,
            "build_dir": build_dir,
        }

    except Exception as e:
        logger.error("Failed to build co-simulation templates", exc_info=True)
        raise e
    finally:
        omc.sendExpression("quit()")


def _run_fast_co_sim_job(
    config: dict, job_params: dict, job_id: int, templates: dict
) -> str:
    """
    快速协同仿真任务执行器 (Compile Once + Rename Restore 模式)。
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]
    co_sim_config = config["co_simulation"]

    base_temp_dir = os.path.abspath(paths_config.get("temp_dir", "temp"))
    job_workspace = os.path.join(base_temp_dir, f"job_{job_id}")
    os.makedirs(job_workspace, exist_ok=True)

    om_bin_path = templates["om_bin"]
    build_source_dir = templates["build_dir"]
    csv_mapping = templates["csv_mapping"]

    # ==========================================================================
    # 核心修复：复制并恢复文件原名 (Strip Prefix)
    # ==========================================================================
    def copy_and_restore_artifacts(src_exe_with_prefix, dest_dir):
        """
        复制构件时移除 'StageX_' 前缀，解决 '找不到 init.xml' 的问题。
        例如: Stage1_Model.exe -> Model.exe
              Stage1_Model_init.xml -> Model_init.xml
        """
        # 1. 获取带前缀的文件名 (e.g., Stage1_CFEDR.Cycle.exe)
        src_basename = os.path.basename(src_exe_with_prefix)

        # 2. 推断前缀 (Stage1_ 或 Stage2_)
        if src_basename.startswith("Stage1_"):
            prefix = "Stage1_"
        elif src_basename.startswith("Stage2_"):
            prefix = "Stage2_"
        else:
            prefix = ""  # 没有前缀，可能是异常情况，但也兼容处理

        # 3. 获取不带扩展名的基础前缀 (用于 glob)
        # e.g., "Stage1_CFEDR.Cycle"
        file_root = os.path.splitext(src_basename)[0]

        # 4. 查找所有相关文件
        artifacts = glob.glob(os.path.join(build_source_dir, f"{file_root}*"))
        ignored_exts = {".c", ".h", ".o", ".cpp", ".log", ".makefile", ".libs", ".json"}

        final_exe_path = ""

        for src in artifacts:
            if not os.path.isfile(src):
                continue
            _, ext = os.path.splitext(src)
            if ext.lower() in ignored_exts:
                continue

            filename = os.path.basename(src)

            # === 关键步骤：剥离前缀 ===
            # 如果文件名以 StageX_ 开头，则去掉它
            if prefix and filename.startswith(prefix):
                original_name = filename[len(prefix) :]
            else:
                original_name = filename

            dst = os.path.join(dest_dir, original_name)

            try:
                shutil.copy(src, dst)

                # 记录恢复原名后的 EXE 路径
                # 判断逻辑：如果当前源文件就是传入的 src_exe_with_prefix
                if filename == src_basename:
                    final_exe_path = dst
            except IOError as e:
                logger.warning(f"Job {job_id}: Failed to copy {filename}: {e}")

        if not final_exe_path:
            raise RuntimeError(f"Failed to restore executable from {src_basename}")

        return final_exe_path

    # 准备环境变量
    env = os.environ.copy()
    if sys.platform == "win32":
        env["PATH"] = om_bin_path + os.pathsep + env["PATH"]

    # 构造 Override
    override_pairs = [f"{k}={v}" for k, v in job_params.items()]
    override_pairs.extend(
        [
            f"stopTime={sim_config['stop_time']}",
            f"stepSize={sim_config['step_size']}",
            "outputFormat=csv",
        ]
    )
    if sim_config.get("variableFilter"):
        override_pairs.append(f"variableFilter={sim_config['variableFilter']}")
    override_str = ",".join(override_pairs)

    try:
        # ======================================================================
        # Phase 1: 运行原始模型 (Stage 1)
        # ======================================================================
        # 复制并改名: Stage1_Cycle.exe -> job_1/Cycle.exe
        s1_exe_source = templates["stage1"]["exe"]
        s1_exe_path = copy_and_restore_artifacts(s1_exe_source, job_workspace)

        primary_input_csv = os.path.join(job_workspace, "primary_inputs.csv")

        # 此时 s1_exe_path 已经是 job_workspace/Cycle.exe
        # 它会自动找到同目录下的 Cycle_init.xml
        subprocess.run(
            [s1_exe_path, "-override", override_str, "-r", primary_input_csv],
            env=env,
            cwd=job_workspace,
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE,
        )

        # 清洗 Stage 1 结果
        if os.path.exists(primary_input_csv):
            df = pd.read_csv(primary_input_csv)
            df.drop_duplicates(subset=["time"], keep="last", inplace=True)
            df.dropna(subset=["time"], inplace=True)
            df.to_csv(primary_input_csv, index=False)
        else:
            raise FileNotFoundError("Stage 1 result not generated")

        # ======================================================================
        # Phase 2: 执行 Handlers
        # ======================================================================
        handlers = co_sim_config.get("handlers", [])
        if not isinstance(handlers, list):
            handlers = [handlers]

        for handler_config in handlers:
            instance_name = handler_config["instance_name"]

            if instance_name not in csv_mapping:
                raise ValueError(f"Unknown instance {instance_name}")

            target_buffer_csv = os.path.join(job_workspace, csv_mapping[instance_name])

            # 加载模块
            module = None
            if "handler_script_path" in handler_config:
                script_path = Path(handler_config["handler_script_path"]).resolve()
                spec = importlib.util.spec_from_file_location(
                    script_path.stem, script_path
                )
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
            elif "handler_module" in handler_config:
                module = importlib.import_module(handler_config["handler_module"])

            if not module:
                raise ImportError(f"Failed to load handler for {instance_name}")

            handler_function = getattr(module, handler_config["handler_function"])

            handler_function(
                temp_input_csv=primary_input_csv,
                temp_output_csv=target_buffer_csv,
                **handler_config.get("params", {}),
            )

            if not os.path.exists(target_buffer_csv):
                raise FileNotFoundError(f"Handler output missing: {target_buffer_csv}")

        # ======================================================================
        # Phase 3: 运行拦截模型 (Stage 2)
        # ======================================================================
        # 复制并改名: Stage2_Cycle.exe -> job_1/Cycle.exe (覆盖之前的 Stage 1 EXE 也没关系)
        s2_exe_source = templates["stage2"]["exe"]
        s2_exe_path = copy_and_restore_artifacts(s2_exe_source, job_workspace)

        final_res_csv = os.path.join(job_workspace, "final_result.csv")

        subprocess.run(
            [s2_exe_path, "-override", override_str, "-r", final_res_csv],
            env=env,
            cwd=job_workspace,
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE,
        )

        if os.path.exists(final_res_csv):
            df = pd.read_csv(final_res_csv)
            df.drop_duplicates(subset=["time"], keep="last", inplace=True)
            df.to_csv(final_res_csv, index=False)
            return final_res_csv
        else:
            raise FileNotFoundError("Stage 2 result not generated")

    except subprocess.CalledProcessError as e:
        logger.error(
            f"Job {job_id} Process Error: {e.stderr.decode() if e.stderr else str(e)}"
        )
        return ""
    except Exception as e:
        logger.error(f"Job {job_id} Error: {str(e)}", exc_info=True)
        return ""


def _build_model_only(config: dict) -> tuple[str, str, str]:
    """
    阶段 1: 仅编译。
    只运行一次，生成 .exe 和 _init.xml，供后续并行任务复用。
    返回: (exe_path, xml_path, om_bin_path)
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]

    # 创建一个专门的构建目录，避免污染项目根目录
    build_dir = os.path.abspath(
        os.path.join(paths_config.get("temp_dir", "temp"), "build")
    )
    os.makedirs(build_dir, exist_ok=True)

    logger.info(f"Building model in {build_dir}...")

    omc = get_om_session()
    try:
        # 1. 切换 OMC 工作目录到 build_dir，确保生成的文件都在这里
        omc.sendExpression(f'cd("{Path(build_dir).as_posix()}")')

        # 2. 加载模型包
        package_path = os.path.abspath(paths_config["package_path"])
        if not load_modelica_package(omc, Path(package_path).as_posix()):
            raise RuntimeError("Failed to load Modelica package during build.")

        # 3. 执行编译 (buildModel)
        model_name = sim_config["model_name"]
        # buildModel 返回元组: (exe_file, xml_file)
        build_result = omc.sendExpression(f"buildModel({model_name})")

        if not build_result or len(build_result) < 2:
            # 尝试从错误日志获取信息
            err = omc.sendExpression("getErrorString()")
            raise RuntimeError(f"Model build failed: {err}")

        exe_name = build_result[0] + ".exe"
        xml_name = build_result[1]

        # 获取绝对路径
        exe_path = os.path.join(build_dir, exe_name)
        xml_path = os.path.join(build_dir, xml_name)

        # 4. 关键：获取 OpenModelica 的 bin 路径 (用于解决 DLL 问题)
        # 我们通过询问 OMC 它的 home 目录在哪里来推断
        om_home = omc.sendExpression("getInstallationDirectoryPath()")
        om_bin_path = os.path.join(om_home, "bin")

        logger.info(f"Model built successfully: {exe_path}")
        return exe_path, xml_path, om_bin_path

    finally:
        omc.sendExpression("quit()")


def _run_fast_subprocess_job(
    job_params: dict,
    job_id: int,
    exe_source: str,
    xml_source: str,
    om_bin_path: str,
    base_temp_dir: str,
    sim_config: dict,
    variable_filter: str = None,
    inplace_execution: bool = False,  # <--- [新增参数] 是否原地执行
) -> str:
    """
    执行单个仿真任务。
    inplace_execution=True: 直接在 build 目录运行 exe，不复制文件 (适合串行)。
    inplace_execution=False: 将 exe 复制到 job 目录运行 (适合并行，防止文件锁)。
    """
    # 1. 准备结果存放目录 (即使不复制exe，我们也需要一个地方放结果csv)
    job_workspace = os.path.join(base_temp_dir, f"job_{job_id}")
    os.makedirs(job_workspace, exist_ok=True)

    # 定义运行时的“当前工作目录” (CWD) 和 “可执行文件路径”
    run_cwd = ""
    run_exe_path = ""

    if inplace_execution:
        # === 模式 A: 原地执行 (串行优化) ===
        # 工作目录保持在 build_artifacts，直接复用源文件
        run_cwd = os.path.dirname(exe_source)
        run_exe_path = exe_source
        # 结果文件使用绝对路径，指向 job_workspace
        result_filename = os.path.abspath(
            os.path.join(job_workspace, f"job_{job_id}_res.csv")
        )
    else:
        # === 模式 B: 隔离执行 (并行安全) ===
        # 工作目录是 job_workspace，需要复制文件
        run_cwd = job_workspace

        # --- 复制逻辑 (含 bin 文件和过滤) ---
        build_dir = os.path.dirname(exe_source)
        model_prefix = os.path.splitext(os.path.basename(exe_source))[0]
        artifacts = glob.glob(os.path.join(build_dir, f"{model_prefix}*"))
        ignored_extensions = {".c", ".h", ".o", ".cpp", ".log", ".makefile", ".libs"}

        run_exe_path = ""
        try:
            for src_file in artifacts:
                if not os.path.isfile(src_file):
                    continue
                _, ext = os.path.splitext(src_file)
                if ext.lower() in ignored_extensions:
                    continue

                dst_file = os.path.join(job_workspace, os.path.basename(src_file))
                shutil.copy(src_file, dst_file)

                if os.path.basename(exe_source) == os.path.basename(src_file):
                    run_exe_path = dst_file
        except IOError as e:
            logger.error(f"Job {job_id}: Copy failed: {e}")
            return ""

        # 结果文件路径 (相对或绝对均可，这里用绝对路径保持一致)
        result_filename = os.path.abspath(
            os.path.join(job_workspace, f"job_{job_id}_res.csv")
        )

    if not run_exe_path:
        logger.error(f"Job {job_id}: Executable path invalid.")
        return ""

    # 2. 构造参数 (Override)
    override_pairs = [f"{k}={v}" for k, v in job_params.items()]
    override_pairs.append(f"stopTime={sim_config['stop_time']}")
    override_pairs.append(f"stepSize={sim_config['step_size']}")
    override_pairs.append("outputFormat=csv")
    if variable_filter:
        # 注意：OpenModelica 的 variableFilter 接受正则表达式
        # 建议始终包含 time，虽然 OM 通常会自动包含它，但显式写上更安全
        # 格式：variableFilter=expr
        override_pairs.append(f"variableFilter={variable_filter}")
    override_str = ",".join(override_pairs)

    # 3. 构造命令
    # 注意：OpenModelica 的 -r 参数支持绝对路径
    cmd = [run_exe_path, "-override", override_str, "-r", result_filename]

    # 4. 环境变量 (DLL)
    env = os.environ.copy()
    if sys.platform == "win32":
        env["PATH"] = om_bin_path + os.pathsep + env["PATH"]

    try:
        # 5. 执行
        # 注意 cwd 的变化：
        # 原地模式下 cwd=build_dir (为了让exe找到同一目录下的 _init.xml 和 .bin)
        # 隔离模式下 cwd=job_workspace
        subprocess.run(
            cmd,
            env=env,
            cwd=run_cwd,
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE,
        )

        if os.path.exists(result_filename):
            try:
                # 仍然建议保留这个清洗步骤，因为 OM 有时会在事件点输出重复时间步
                df = pd.read_csv(result_filename)
                df.drop_duplicates(subset=["time"], keep="last", inplace=True)
                df.to_csv(result_filename, index=False)
                return result_filename
            except Exception as e:
                logger.warning(f"Job {job_id}: Cleaning failed: {e}")
                return result_filename
        else:
            return ""

    except subprocess.CalledProcessError as e:
        logger.error(f"Job {job_id} failed: {e.stderr.decode()}")
        return ""
    except Exception as e:
        logger.error(f"Job {job_id} unexpected error: {str(e)}")
        return ""


def _run_co_simulation(config: dict, job_params: dict, job_id: int = 0) -> str:
    """Runs the full co-simulation workflow in an isolated directory.

    This function sets up a self-contained workspace for a single co-simulation
    job to ensure thread safety. It copies the model package, handles asset
    files, generates an interceptor model, and executes a two-stage
    simulation: first to get primary inputs, and second to run the final
    simulation with the intercepted model.

    Args:
        config: The main configuration dictionary.
        job_params: A dictionary of parameters specific to this job.
        job_id: A unique identifier for the job, used for workspace naming. Defaults to 0.

    Returns:
        The path to the final simulation result file, or an empty string if the simulation failed.

    Note:
        Creates isolated workspace in temp_dir/job_{job_id}. Supports both single-file
        and multi-file Modelica packages. Handles external interceptor handlers. Cleans
        up workspace after completion. Two-stage simulation: stage 1 generates input CSV,
        stage 2 runs with interceptor model.
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]

    base_temp_dir = os.path.abspath(paths_config.get("temp_dir", "temp"))
    # The temp_dir from the config is now the self-contained workspace's temp folder.
    job_workspace = os.path.join(base_temp_dir, f"job_{job_id}")
    os.makedirs(job_workspace, exist_ok=True)

    omc = None

    try:
        original_package_path = os.path.abspath(paths_config["package_path"])

        # Determine if it's a single-file or multi-file package and copy accordingly.
        if os.path.isfile(original_package_path) and not original_package_path.endswith(
            "package.mo"
        ):
            # SINGLE-FILE: Copy the single .mo file into the root of the job_workspace.
            isolated_package_path = os.path.join(
                job_workspace, os.path.basename(original_package_path)
            )
            shutil.copy(original_package_path, isolated_package_path)
            logger.info(
                "Copied single-file package",
                extra={
                    "job_id": job_id,
                    "source_path": original_package_path,
                    "destination_path": isolated_package_path,
                },
            )
        else:
            # MULTI-FILE: Copy the entire package directory.
            # This handles both a directory path and a path to a package.mo file.
            if os.path.isfile(original_package_path):
                original_package_dir = os.path.dirname(original_package_path)
            else:  # It's a directory
                original_package_dir = original_package_path

            package_dir_name = os.path.basename(original_package_dir)
            isolated_package_dir = os.path.join(job_workspace, package_dir_name)

            if os.path.exists(isolated_package_dir):
                shutil.rmtree(isolated_package_dir)
            shutil.copytree(original_package_dir, isolated_package_dir)

            # Reconstruct the path to the main package file inside the new isolated directory
            if os.path.isfile(original_package_path):
                isolated_package_path = os.path.join(
                    isolated_package_dir, os.path.basename(original_package_path)
                )
            else:  # path was a directory, so we assume package.mo
                isolated_package_path = os.path.join(isolated_package_dir, "package.mo")

            logger.info(
                "Copied multi-file package",
                extra={
                    "job_id": job_id,
                    "source_dir": original_package_dir,
                    "destination_dir": isolated_package_dir,
                },
            )

        isolated_temp_dir = job_workspace
        results_dir = os.path.abspath(paths_config["results_dir"])
        os.makedirs(results_dir, exist_ok=True)

        # Parse co_simulation config - new format with mode at top level
        co_sim_config = config["co_simulation"]
        mode = co_sim_config.get("mode", "interceptor")  # Get mode from top level
        handlers = co_sim_config.get("handlers", [])  # Get handlers array

        # Validate that handlers is a list
        if not isinstance(handlers, list):
            handlers = [handlers]

        model_name = sim_config["model_name"]
        stop_time = sim_config["stop_time"]
        step_size = sim_config["step_size"]

        omc = get_om_session()
        if not load_modelica_package(omc, Path(isolated_package_path).as_posix()):
            raise RuntimeError(
                f"Failed to load Modelica package at {isolated_package_path}"
            )

        # Handle copying of any additional asset directories specified with a '_path' suffix
        for handler_config in handlers:
            if "params" in handler_config:
                # Iterate over a copy of items since we are modifying the dict
                for param_key, param_value in list(handler_config["params"].items()):
                    if isinstance(param_value, str) and param_key.endswith("_path"):
                        original_asset_path_str = param_value

                        # Paths in config are relative to project root. We need the absolute path.
                        original_asset_path = Path(
                            os.path.abspath(original_asset_path_str)
                        )
                        original_asset_dir = original_asset_path.parent

                        if not original_asset_dir.exists():
                            logger.warning(
                                f"Asset directory '{original_asset_dir}' for parameter '{param_key}' not found. Skipping copy."
                            )
                            continue

                        asset_dir_name = original_asset_dir.name
                        dest_dir = Path(job_workspace) / asset_dir_name

                        # Copy the directory only if it hasn't been copied already
                        if not dest_dir.exists():
                            shutil.copytree(original_asset_dir, dest_dir)
                            logger.info(
                                "Copied asset directory",
                                extra={
                                    "job_id": job_id,
                                    "source_dir": original_asset_dir,
                                    "destination_dir": dest_dir,
                                },
                            )

                        # Update the path in the config to point to the new location
                        new_asset_path = dest_dir / original_asset_path.name
                        handler_config["params"][param_key] = new_asset_path.as_posix()
                        logger.info(
                            "Updated asset parameter path",
                            extra={
                                "job_id": job_id,
                                "parameter_key": param_key,
                                "new_path": handler_config["params"][param_key],
                            },
                        )

        all_input_vars = []
        for handler_config in handlers:
            submodel_name = handler_config["submodel_name"]
            instance_name = handler_config["instance_name"]
            logger.info(
                "Identifying input ports for submodel",
                extra={
                    "job_id": job_id,
                    "submodel_name": submodel_name,
                },
            )
            components = omc.sendExpression(f"getComponents({submodel_name})")
            input_ports = [
                {"name": c[1], "dim": int(c[11][0]) if c[11] else 1}
                for c in components
                if c[0] == "Modelica.Blocks.Interfaces.RealInput"
            ]
            if not input_ports:
                logger.warning(f"No RealInput ports found in {submodel_name}.")
                continue

            logger.info(
                "Found input ports for instance",
                extra={
                    "job_id": job_id,
                    "instance_name": instance_name,
                    "input_ports": [p["name"] for p in input_ports],
                },
            )
            for port in input_ports:
                full_name = f"{instance_name}.{port['name']}".replace(".", "\\.")
                if port["dim"] > 1:
                    full_name += f"\\[[1-{port['dim']}]\\]"
                all_input_vars.append(full_name)

        variable_filter = "time|" + "|".join(all_input_vars)

        mod = ModelicaSystem(
            fileName=Path(isolated_package_path).as_posix(),
            modelName=model_name,
            variableFilter=variable_filter,
        )
        mod.setSimulationOptions(
            [f"stopTime={stop_time}", f"stepSize={step_size}", "outputFormat=csv"]
        )

        param_settings = [
            format_parameter_value(name, value) for name, value in job_params.items()
        ]
        if param_settings:
            logger.info(
                "Applying parameters for job",
                extra={
                    "job_id": job_id,
                    "param_settings": param_settings,
                },
            )
            mod.setParameters(param_settings)

        primary_result_filename = get_unique_filename(
            isolated_temp_dir, "primary_inputs.csv"
        )
        mod.simulate(resultfile=Path(primary_result_filename).as_posix())

        # Clean up the simulation result file
        if os.path.exists(primary_result_filename):
            try:
                df = pd.read_csv(primary_result_filename)
                df.drop_duplicates(subset=["time"], keep="last", inplace=True)
                df.dropna(subset=["time"], inplace=True)
                df.to_csv(primary_result_filename, index=False)
            except Exception as e:
                logger.warning(
                    "Failed to clean primary result file",
                    extra={
                        "job_id": job_id,
                        "file_path": primary_result_filename,
                        "error": str(e),
                    },
                )

        interception_configs = []
        for handler_config in handlers:
            handler_function_name = handler_config["handler_function"]
            module = None

            # New method: Load from a direct script path
            if "handler_script_path" in handler_config:
                script_path_str = handler_config["handler_script_path"]
                script_path = Path(script_path_str).resolve()
                module_name = script_path.stem

                logger.info(
                    "Loading co-simulation handler from script path",
                    extra={
                        "job_id": job_id,
                        "script_path": str(script_path),
                        "function": handler_function_name,
                    },
                )

                if not script_path.is_file():
                    raise FileNotFoundError(
                        f"Co-simulation handler script not found at {script_path}"
                    )

                spec = importlib.util.spec_from_file_location(module_name, script_path)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                else:
                    raise ImportError(
                        f"Could not create module spec from script {script_path}"
                    )

            # Old method: Load from module name (backward compatibility)
            elif "handler_module" in handler_config:
                module_name = handler_config["handler_module"]
                logger.info(
                    "Loading co-simulation handler from module",
                    extra={
                        "job_id": job_id,
                        "module_name": module_name,
                        "function": handler_function_name,
                    },
                )
                module = importlib.import_module(module_name)

            else:
                raise KeyError(
                    "Handler config must contain either 'script_path' or 'handler_module'"
                )

            if not module:
                raise ImportError("Failed to load co-simulation handler module.")

            handler_function = getattr(module, handler_function_name)
            instance_name = handler_config["instance_name"]

            co_sim_output_filename = get_unique_filename(
                isolated_temp_dir, f"{instance_name}_outputs.csv"
            )

            output_placeholder = handler_function(
                temp_input_csv=primary_result_filename,
                temp_output_csv=co_sim_output_filename,
                **handler_config.get("params", {}),
            )

            interception_config = {
                "submodel_name": handler_config["submodel_name"],
                "instance_name": handler_config["instance_name"],
                "csv_uri": Path(os.path.abspath(co_sim_output_filename)).as_posix(),
                "output_placeholder": output_placeholder,
            }

            # Add mode from top-level co_simulation config
            interception_config["mode"] = mode

            interception_configs.append(interception_config)

        intercepted_model_paths = integrate_interceptor_model(
            package_path=isolated_package_path,
            model_name=model_name,
            interception_configs=interception_configs,
        )

        verif_config = config["simulation"]["variableFilter"]
        logger.info("Proceeding with Final simulation.", extra={"job_id": job_id})

        # Use mode from top-level config
        if mode == "replacement":
            # For direct replacement, the system model name stays the same
            logger.info(
                "Using direct replacement mode, system model unchanged",
                extra={"job_id": job_id},
            )
            final_model_name = model_name
            final_model_file = isolated_package_path
        else:
            # For interceptor mode, load the interceptor models and use modified system
            for model_path in intercepted_model_paths["interceptor_model_paths"]:
                omc.sendExpression(f"""loadFile("{Path(model_path).as_posix()}")""")
            omc.sendExpression(
                f"""loadFile("{Path(intercepted_model_paths["system_model_path"]).as_posix()}")"""
            )

            package_name, original_system_name = model_name.split(".")
            final_model_name = f"{package_name}.{original_system_name}_Intercepted"
            final_model_file = (
                Path(intercepted_model_paths["system_model_path"]).as_posix()
                if os.path.isfile(isolated_package_path)
                and not original_package_path.endswith("package.mo")
                else Path(isolated_package_path).as_posix()
            )

        verif_mod = ModelicaSystem(
            fileName=final_model_file,
            modelName=final_model_name,
            variableFilter=verif_config,
        )
        verif_mod.setSimulationOptions(
            [f"stopTime={stop_time}", f"stepSize={step_size}", "outputFormat=csv"]
        )
        if param_settings:
            verif_mod.setParameters(param_settings)

        default_result_path = get_unique_filename(
            job_workspace, "co_simulation_results.csv"
        )
        verif_mod.simulate(resultfile=Path(default_result_path).as_posix())

        # Clean up the simulation result file
        if os.path.exists(default_result_path):
            try:
                df = pd.read_csv(default_result_path)
                df.drop_duplicates(subset=["time"], keep="last", inplace=True)
                df.dropna(subset=["time"], inplace=True)
                df.to_csv(default_result_path, index=False)
            except Exception as e:
                logger.warning(
                    "Failed to clean final co-simulation result file",
                    extra={
                        "job_id": job_id,
                        "file_path": default_result_path,
                        "error": str(e),
                    },
                )

        if not os.path.exists(default_result_path):
            raise FileNotFoundError(
                f"Simulation for job {job_id} failed to produce a result file at {default_result_path}"
            )

        # Return the path to the result file inside the temporary workspace
        return Path(default_result_path).as_posix()
    except Exception:
        logger.error(
            "Co-simulation workflow failed", exc_info=True, extra={"job_id": job_id}
        )
        return ""
    finally:
        if omc:
            omc.sendExpression("quit()")
            logger.info("Closed OMPython session", extra={"job_id": job_id})

        if not sim_config.get("keep_temp_files", True):
            if os.path.exists(job_workspace):
                shutil.rmtree(job_workspace)
                logger.info(
                    "Cleaned up job workspace",
                    extra={"job_id": job_id, "workspace": job_workspace},
                )


def _run_single_job(config: dict, job_params: dict, job_id: int = 0) -> str:
    """Executes a single standard simulation job in an isolated workspace.

    This function sets up a dedicated workspace for one simulation run,
    initializes an OpenModelica session, sets the specified model parameters,
    runs the simulation, and cleans up the result file.

    Args:
        config: The main configuration dictionary.
        job_params: A dictionary of parameters specific to this job.
        job_id: A unique identifier for the job. Defaults to 0.

    Returns:
        The path to the simulation result file, or an empty string on failure.

    Note:
        Creates isolated workspace in temp_dir/job_{job_id}. Cleans result file by
        removing duplicate/NaN time values. Cleans up workspace unless keep_temp_files
        is True. Uses CSV output format with configurable stopTime and stepSize.
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]

    base_temp_dir = os.path.abspath(paths_config.get("temp_dir", "temp"))
    # The temp_dir from the config is now the self-contained workspace's temp folder.
    job_workspace = os.path.join(base_temp_dir, f"job_{job_id}")
    os.makedirs(job_workspace, exist_ok=True)

    logger.info(
        "Starting single job",
        extra={"job_id": job_id, "job_params": job_params},
    )
    omc = None
    try:
        omc = get_om_session()
        package_path = os.path.abspath(paths_config["package_path"])
        if not load_modelica_package(omc, Path(package_path).as_posix()):
            raise RuntimeError(f"Job {job_id}: Failed to load Modelica package.")

        mod = ModelicaSystem(
            fileName=Path(package_path).as_posix(),
            modelName=sim_config["model_name"],
            variableFilter=sim_config["variableFilter"],
        )
        mod.setSimulationOptions(
            [
                f"stopTime={sim_config['stop_time']}",
                "tolerance=1e-6",
                "outputFormat=csv",
                f"stepSize={sim_config['step_size']}",
            ]
        )
        param_settings = [
            format_parameter_value(name, value) for name, value in job_params.items()
        ]
        if param_settings:
            mod.setParameters(param_settings)

        default_result_file = f"job_{job_id}_simulation_results.csv"
        result_path = Path(job_workspace) / default_result_file

        mod.simulate(resultfile=Path(result_path).as_posix())

        # Clean up the simulation result file
        if os.path.exists(result_path):
            try:
                df = pd.read_csv(result_path)
                df.drop_duplicates(subset=["time"], keep="last", inplace=True)
                df.dropna(subset=["time"], inplace=True)
                df.to_csv(result_path, index=False)
            except Exception as e:
                logger.warning(
                    "Failed to clean result file",
                    extra={
                        "job_id": job_id,
                        "file_path": result_path,
                        "error": str(e),
                    },
                )

        if not result_path.is_file():
            raise FileNotFoundError(
                f"Simulation for job {job_id} failed to produce result file at {result_path}"
            )

        logger.info(
            "Job finished successfully",
            extra={"job_id": job_id, "result_path": str(result_path)},
        )
        return str(result_path)
    except Exception:
        logger.error("Job failed", exc_info=True, extra={"job_id": job_id})
        return ""
    finally:
        if omc:
            omc.sendExpression("quit()")


def _run_sequential_sweep(config: dict, jobs: List[Dict[str, Any]]) -> List[str]:
    """Executes a parameter sweep sequentially.

    This function runs a series of simulation jobs one after another, reusing
    the same OpenModelica session for efficiency. Intermediate results for each
    job are saved to a dedicated job workspace.

    Args:
        config: The main configuration dictionary.
        jobs: A list of job parameter dictionaries to execute.

    Returns:
        A list of paths to the result files for each job. Failed jobs will have
        an empty string as their path.

    Note:
        Reuses single OMPython session for all jobs. Cleans result files by removing
        duplicate/NaN time values. Cleans up workspaces unless keep_temp_files is True.
        More efficient than parallel mode for small job counts.
    """
    paths_config = config["paths"]
    sim_config = config["simulation"]

    base_temp_dir = os.path.abspath(paths_config.get("temp_dir", "temp"))
    # The temp_dir is now the self-contained workspace's temp folder.
    os.makedirs(base_temp_dir, exist_ok=True)

    logger.info(
        "Running sequential sweep",
        extra={
            "mode": "sequential",
            "intermediate_files_dir": base_temp_dir,
        },
    )

    omc = None
    result_paths = []
    try:
        omc = get_om_session()
        package_path = os.path.abspath(paths_config["package_path"])
        if not load_modelica_package(omc, Path(package_path).as_posix()):
            raise RuntimeError("Failed to load Modelica package for sequential sweep.")

        mod = ModelicaSystem(
            fileName=Path(package_path).as_posix(),
            modelName=sim_config["model_name"],
            variableFilter=sim_config["variableFilter"],
        )

        mod.setSimulationOptions(
            [
                f"stopTime={sim_config['stop_time']}",
                "tolerance=1e-6",
                "outputFormat=csv",
                f"stepSize={sim_config['step_size']}",
            ]
        )
        # mod.buildModel()

        for i, job_params in enumerate(jobs):
            try:
                logger.info(
                    "Running sequential job",
                    extra={
                        "job_index": f"{i+1}/{len(jobs)}",
                        "job_params": job_params,
                    },
                )
                param_settings = [
                    format_parameter_value(name, value)
                    for name, value in job_params.items()
                ]
                if param_settings:
                    mod.setParameters(param_settings)

                job_workspace = os.path.join(base_temp_dir, f"job_{i+1}")
                os.makedirs(job_workspace, exist_ok=True)
                result_filename = f"job_{i+1}_simulation_results.csv"
                result_file_path = os.path.join(job_workspace, result_filename)

                mod.simulate(resultfile=Path(result_file_path).as_posix())

                # Clean up the simulation result file
                if os.path.exists(result_file_path):
                    try:
                        df = pd.read_csv(result_file_path)
                        df.drop_duplicates(subset=["time"], keep="last", inplace=True)
                        df.dropna(subset=["time"], inplace=True)
                        df.to_csv(result_file_path, index=False)
                    except Exception as e:
                        logger.warning(
                            "Failed to clean result file",
                            extra={
                                "job_index": i + 1,
                                "file_path": result_file_path,
                                "error": str(e),
                            },
                        )

                logger.info(
                    "Sequential job finished successfully",
                    extra={
                        "job_index": i + 1,
                        "result_path": result_file_path,
                    },
                )
                result_paths.append(result_file_path)
            except Exception:
                logger.error(
                    "Sequential job failed", exc_info=True, extra={"job_index": i + 1}
                )
                result_paths.append("")

        return result_paths
    except Exception:
        logger.error("Sequential sweep setup failed", exc_info=True)
        return [""] * len(jobs)
    finally:
        if omc:
            omc.sendExpression("quit()")


def _run_post_processing(
    config: Dict[str, Any], results_df: pd.DataFrame, post_processing_output_dir: str
) -> None:
    """Dynamically loads and runs post-processing modules.

    This function iterates through the post-processing tasks defined in the
    configuration. For each task, it dynamically loads the specified module
    (from a module name or a script path) and executes the target function,
    passing the results DataFrame and other parameters to it.

    Args:
        config: The main configuration dictionary.
        results_df: The combined DataFrame of simulation results.
        post_processing_output_dir: The directory to save any output from the tasks.

    Note:
        Supports two loading methods: 'script_path' for direct .py files, or 'module'
        for installed packages. Creates output_dir if it doesn't exist. Passes results_df,
        output_dir, and user-specified params to each task function. Logs errors for
        failed tasks but continues with remaining tasks.
    """
    post_processing_configs = config.get("post_processing")
    if not post_processing_configs:
        logger.info("No post-processing task configured, skipping this step.")
        return

    logger.info("Starting post-processing phase")

    post_processing_dir = post_processing_output_dir
    os.makedirs(post_processing_dir, exist_ok=True)
    logger.info(
        "Post-processing report will be saved",
        extra={"output_dir": post_processing_dir},
    )

    for i, task_config in enumerate(post_processing_configs):
        try:
            function_name = task_config["function"]
            params = task_config.get("params", {})
            module = None

            # New method: Load from a direct script path
            if "script_path" in task_config:
                script_path_str = task_config["script_path"]
                script_path = Path(script_path_str).resolve()
                module_name = script_path.stem

                logger.info(
                    "Running post-processing task from script path",
                    extra={
                        "task_index": i + 1,
                        "script_path": str(script_path),
                        "function": function_name,
                    },
                )

                if not script_path.is_file():
                    logger.error(
                        "Post-processing script not found",
                        extra={"path": str(script_path)},
                    )
                    continue

                spec = importlib.util.spec_from_file_location(module_name, script_path)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                else:
                    logger.error(
                        "Could not create module spec from script",
                        extra={"path": str(script_path)},
                    )
                    continue

            # Old method: Load from module name (backward compatibility)
            elif "module" in task_config:
                module_name = task_config["module"]
                logger.info(
                    "Running post-processing task from module",
                    extra={
                        "task_index": i + 1,
                        "module_name": module_name,
                        "function": function_name,
                    },
                )
                module = importlib.import_module(module_name)

            else:
                logger.warning(
                    "Post-processing task is missing 'script_path' or 'module' key. Skipping.",
                    extra={"task_index": i + 1},
                )
                continue

            if module:
                post_processing_func = getattr(module, function_name)
                post_processing_func(
                    results_df=results_df, output_dir=post_processing_dir, **params
                )
            else:
                logger.error(
                    "Failed to load post-processing module.",
                    extra={"task_index": i + 1},
                )

        except Exception:
            logger.error(
                "Post-processing task failed",
                exc_info=True,
                extra={"task_index": i + 1},
            )
    logger.info("Post-processing phase ended")


def run_simulation(config: Dict[str, Any]) -> None:
    """Orchestrates the main simulation workflow (Optimized)."""

    # Generate simulation jobs from parameters
    jobs = generate_simulation_jobs(config.get("simulation_parameters", {}))

    try:
        results_dir = os.path.abspath(config["paths"]["results_dir"])
        temp_dir = os.path.abspath(config["paths"].get("temp_dir", "temp"))
    except KeyError as e:
        logger.error(f"Missing required path key in configuration file: {e}")
        sys.exit(1)

    simulation_results = {}
    use_concurrent = config["simulation"].get("concurrent", False)
    max_workers = config["simulation"].get("max_workers", os.cpu_count())
    variable_filter = config["simulation"].get("variableFilter", None)  # 获取配置

    try:
        # ==============================================================================
        # 分支 A: 标准仿真 (Standard Simulation) -> 走优化路径 (Compile Once, Run Parallel)
        # ==============================================================================
        if config.get("co_simulation") is None:
            logger.info("Starting Standard Simulation (Optimized Mode)")

            # 1. 预编译阶段：生成 master exe
            master_exe, master_xml, om_bin = _build_model_only(config)

            # 2. 运行阶段：并行/串行分发
            if use_concurrent:
                logger.info(
                    "Running jobs concurrently (Subprocess)",
                    extra={"max_workers": max_workers, "job_count": len(jobs)},
                )
                # ThreadPoolExecutor 足够了，因为 subprocess.run 释放了 GIL 且也是调用外部进程
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    future_to_job = {
                        executor.submit(
                            _run_fast_subprocess_job,
                            job_params,
                            i + 1,
                            master_exe,
                            master_xml,
                            om_bin,
                            temp_dir,
                            config["simulation"],
                            variable_filter=variable_filter,  # <--- 传入参数
                        ): job_params
                        for i, job_params in enumerate(jobs)
                    }

                    for future in concurrent.futures.as_completed(future_to_job):
                        job_params = future_to_job[future]
                        try:
                            result_path = future.result()
                            if result_path:
                                simulation_results[
                                    tuple(sorted(job_params.items()))
                                ] = result_path
                        except Exception as exc:
                            logger.error(
                                f"Job {job_params} generated an exception: {exc}"
                            )
            else:
                # === 串行模式 ===
                # 启用优化：原地执行 (inplace_execution=True)
                logger.info("Running SEQUENTIAL jobs (In-Place Optimization Mode)...")
                for i, job_params in enumerate(jobs):
                    result_path = _run_fast_subprocess_job(
                        job_params,
                        i + 1,
                        master_exe,
                        master_xml,
                        om_bin,
                        temp_dir,
                        config["simulation"],
                        variable_filter=variable_filter,
                        inplace_execution=True,  # <--- 串行模式：原地执行，极速！
                    )
                    if result_path:
                        simulation_results[tuple(sorted(job_params.items()))] = (
                            result_path
                        )

        # ==============================================================================
        # 分支 B: 协同仿真 (Co-simulation) -> 保持原有逻辑 (Structure changes dynamically)
        # ==============================================================================
        else:
            # Phase 1: 构建模板 (只做一次!)
            templates = _build_co_sim_templates(config)
            if use_concurrent:
                # === 协同仿真优化路径 ===
                logger.info("Starting Co-Simulation (Optimized Compile-Once Mode)")

                # Phase 2: 并行分发
                # 此时必须使用 ThreadPool (因为是 subprocess 调用) 或 ProcessPool
                # 由于涉及 Python Handler (可能 CPU 密集)，ProcessPool 更安全，
                # 但为了避免 Pickle 问题，且 subprocess 占大头，ThreadPool 也可以尝试。
                # 推荐 ProcessPoolExecutor 用于 Co-Sim。

                with concurrent.futures.ProcessPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    future_to_job = {
                        executor.submit(
                            _run_fast_co_sim_job,  # 新的快速函数
                            config,
                            job_params,
                            i + 1,
                            templates,
                        ): job_params
                        for i, job_params in enumerate(jobs)
                    }
                for future in concurrent.futures.as_completed(future_to_job):
                    job_params = future_to_job[future]
                    try:
                        result_path = future.result()
                        if result_path:
                            simulation_results[tuple(sorted(job_params.items()))] = (
                                result_path
                            )
                    except Exception as exc:
                        logger.error(f"Co-sim job failed: {exc}", exc_info=True)
            else:
                logger.info("Starting Co-simulation", extra={"mode": "SEQUENTIAL"})
                for i, job_params in enumerate(jobs):
                    result_path = _run_fast_co_sim_job(
                        config, job_params, job_id=i + 1, templates=templates
                    )
                    if result_path:
                        simulation_results[tuple(sorted(job_params.items()))] = (
                            result_path
                        )

    except Exception as e:
        raise RuntimeError("Failed to run simulation workflow", e)

    # --- Result Handling (保持原有逻辑不变) ---
    # 下面的代码负责合并 CSV，与之前完全一致，不需要修改

    run_results_dir = results_dir
    os.makedirs(run_results_dir, exist_ok=True)

    logger.info("Processing jobs and combining results", extra={"num_jobs": len(jobs)})

    all_dfs = []
    time_df_added = False

    for job_params in jobs:
        job_key = tuple(sorted(job_params.items()))
        result_path = simulation_results.get(job_key)

        if not result_path or not os.path.exists(result_path):
            continue

        try:
            df = pd.read_csv(result_path)
            if not time_df_added and "time" in df.columns:
                all_dfs.append(df[["time"]])
                time_df_added = True

            param_string = "&".join([f"{k}={v}" for k, v in job_params.items()])
            data_columns = df.drop(columns=["time"], errors="ignore")
            rename_mapping = {
                col: f"{col}&{param_string}" if param_string else col
                for col in data_columns.columns
            }
            all_dfs.append(data_columns.rename(columns=rename_mapping))
        except Exception as e:
            logger.warning(f"Failed to merge result {result_path}: {e}")

    combined_df = None
    if all_dfs:
        combined_df = pd.concat(all_dfs, axis=1)
    else:
        combined_df = pd.DataFrame()

    if combined_df is not None and not combined_df.empty:
        filename = "simulation_result.csv" if len(jobs) == 1 else "sweep_results.csv"
        combined_csv_path = get_unique_filename(run_results_dir, filename)
        combined_df.to_csv(combined_csv_path, index=False)
        logger.info("Combined results saved", extra={"file_path": combined_csv_path})

        # Post-Processing
        if config.get("run_timestamp"):
            top_level_post_dir = os.path.join(
                os.path.abspath(config["run_timestamp"]), "post_processing"
            )
            _run_post_processing(config, combined_df, top_level_post_dir)
    else:
        logger.warning("No valid results found to combine")

    # --- Cleanup ---
    if not config["simulation"].get("keep_temp_files", True):
        try:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                os.makedirs(temp_dir)
        except Exception as e:
            logger.error(f"Error cleaning temp dir: {e}")


def _run_simulation(config: Dict[str, Any]) -> None:
    """Orchestrates the main simulation workflow.

    This function serves as the primary orchestrator for running simulations.
    It generates jobs from parameters, executes them (concurrently or sequentially,
    as standard or co-simulations), merges the results into a single DataFrame,
    and triggers any configured post-processing steps.

    Args:
        config: The main configuration dictionary for the run.

    Note:
        Supports concurrent (ThreadPoolExecutor) and sequential execution modes.
        Co-simulations use ProcessPoolExecutor for better isolation. Merges all job
        results into single CSV with parameter-labeled columns. Triggers post-processing
        tasks if configured. Results saved to paths.results_dir.
    """
    jobs = generate_simulation_jobs(config.get("simulation_parameters", {}))

    try:
        results_dir = os.path.abspath(config["paths"]["results_dir"])
    except KeyError as e:
        logger.error(f"Missing required path key in configuration file: {e}")
        sys.exit(1)

    simulation_results = {}
    use_concurrent = config["simulation"].get("concurrent", False)

    try:
        max_workers = config["simulation"].get("max_workers", os.cpu_count())
        if config.get("co_simulation") is None:
            if use_concurrent:
                logger.info(
                    "Starting simulation",
                    extra={
                        "mode": "CONCURRENT",
                        "max_workers": max_workers,
                    },
                )
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    future_to_job = {
                        executor.submit(
                            _run_single_job, config, job_params, i + 1
                        ): job_params
                        for i, job_params in enumerate(jobs)
                    }
                    for future in concurrent.futures.as_completed(future_to_job):
                        job_params = future_to_job[future]
                        try:
                            result_path = future.result()
                            if result_path:
                                simulation_results[
                                    tuple(sorted(job_params.items()))
                                ] = result_path
                        except Exception as exc:
                            logger.error(
                                f"Job for {job_params} generated an exception: {exc}",
                                exc_info=True,
                            )
            else:
                logger.info("Starting simulation", extra={"mode": "SEQUENTIAL"})
                result_paths = _run_sequential_sweep(config, jobs)
                for i, result_path in enumerate(result_paths):
                    if result_path:
                        simulation_results[tuple(sorted(jobs[i].items()))] = result_path
        else:
            if use_concurrent:
                logger.info(
                    "Starting co-simulation",
                    extra={
                        "mode": "CONCURRENT",
                        "max_workers": max_workers,
                    },
                )

                with concurrent.futures.ProcessPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    future_to_job = {
                        executor.submit(
                            _run_co_simulation, config, job_params, job_id=i + 1
                        ): job_params
                        for i, job_params in enumerate(jobs)
                    }

                    for future in concurrent.futures.as_completed(future_to_job):
                        job_params = future_to_job[future]
                        try:
                            result_path = future.result()
                            if result_path:
                                simulation_results[
                                    tuple(sorted(job_params.items()))
                                ] = result_path
                                logger.info(
                                    "Successfully finished co-simulation job",
                                    extra={
                                        "job_params": job_params,
                                    },
                                )
                            else:
                                logger.warning(
                                    "Co-simulation job did not return a result path",
                                    extra={
                                        "job_params": job_params,
                                    },
                                )
                        except Exception as exc:
                            logger.error(
                                "Co-simulation job generated an exception",
                                exc_info=True,
                                extra={
                                    "job_params": job_params,
                                    "exception": str(exc),
                                },
                            )
            else:
                logger.info("Starting co-simulation", extra={"mode": "SEQUENTIAL"})
                for i, job_params in enumerate(jobs):
                    job_id = i + 1
                    logger.info(
                        "Starting Sequential Co-simulation Job",
                        extra={
                            "job_index": f"{job_id}/{len(jobs)}",
                        },
                    )
                    try:
                        result_path = _run_co_simulation(
                            config, job_params, job_id=job_id
                        )
                        if result_path:
                            simulation_results[tuple(sorted(job_params.items()))] = (
                                result_path
                            )
                            logger.info(
                                "Successfully finished co-simulation job",
                                extra={
                                    "job_params": job_params,
                                },
                            )
                        else:
                            logger.warning(
                                "Co-simulation job did not return a result path",
                                extra={
                                    "job_params": job_params,
                                },
                            )
                    except Exception as exc:
                        logger.error(
                            "Co-simulation job generated an exception",
                            exc_info=True,
                            extra={
                                "job_params": job_params,
                                "exception": str(exc),
                            },
                        )
                    logger.info(
                        "Finished Sequential Co-simulation Job",
                        extra={
                            "job_index": f"{job_id}/{len(jobs)}",
                        },
                    )
    except Exception as e:
        raise RuntimeError("Failed to run simulation", e)

    # --- Result Handling ---
    # The simulation_results dictionary now contains paths to results inside temporary job workspaces.
    # The results_dir from the config is now the self-contained workspace's results folder.
    run_results_dir = results_dir
    os.makedirs(run_results_dir, exist_ok=True)

    # Unified result processing for both single and multiple jobs
    logger.info(
        "Processing jobs and combining results",
        extra={
            "num_jobs": len(jobs),
        },
    )
    combined_df = None

    all_dfs = []
    time_df_added = False

    for job_params in jobs:
        job_key = tuple(sorted(job_params.items()))
        result_path = simulation_results.get(job_key)

        if not result_path or not os.path.exists(result_path):
            logger.warning(
                "Job produced no result file",
                extra={
                    "job_params": job_params,
                },
            )
            continue

        # Read the current job's result file
        df = pd.read_csv(result_path)

        # From the very first valid DataFrame, grab the 'time' column
        if not time_df_added and "time" in df.columns:
            all_dfs.append(df[["time"]])
            time_df_added = True

        # Prepare the parameter string for column renaming
        param_string = "&".join([f"{k}={v}" for k, v in job_params.items()])

        # Isolate the data columns (everything except 'time')
        data_columns = df.drop(columns=["time"], errors="ignore")

        # Create a dictionary to map old column names to new ones
        # e.g., {'voltage': 'voltage&param1=A&param2=B'}
        rename_mapping = {
            col: f"{col}&{param_string}" if param_string else col
            for col in data_columns.columns
        }

        # Rename the columns and add the resulting DataFrame to our list
        all_dfs.append(data_columns.rename(columns=rename_mapping))

    # Concatenate all the DataFrames in the list along the columns axis (axis=1)
    if all_dfs:
        combined_df = pd.concat(all_dfs, axis=1)
    else:
        combined_df = pd.DataFrame()  # Or None, as you had before

    if combined_df is not None and not combined_df.empty:
        if len(jobs) == 1:
            # For single job, save as simulation_result.csv
            combined_csv_path = get_unique_filename(
                run_results_dir, "simulation_result.csv"
            )
        else:
            # For multiple jobs, save as sweep_results.csv
            combined_csv_path = get_unique_filename(
                run_results_dir, "sweep_results.csv"
            )

        combined_df.to_csv(combined_csv_path, index=False)
        logger.info(
            "Combined results saved",
            extra={
                "file_path": combined_csv_path,
            },
        )
    else:
        logger.warning("No valid results found to combine")

    # --- Post-Processing ---
    if combined_df is not None:
        # Calculate the top-level post-processing directory
        top_level_run_workspace = os.path.abspath(config["run_timestamp"])
        top_level_post_processing_dir = os.path.join(
            top_level_run_workspace, "post_processing"
        )
        _run_post_processing(config, combined_df, top_level_post_processing_dir)
    else:
        logger.warning("No simulation results generated, skipping post-processing")

    # --- Final Cleanup ---
    # The primary cleanup of job workspaces is handled by the `finally` block in `_run_co_simulation`.
    # This is an additional safeguard.
    if not config["simulation"].get("keep_temp_files", True):
        temp_dir_path = os.path.abspath(config["paths"].get("temp_dir", "temp"))
        logger.info(
            "Cleaning up temporary directory",
            extra={
                "directory": temp_dir_path,
            },
        )
        if os.path.exists(temp_dir_path):
            try:
                shutil.rmtree(temp_dir_path)
                os.makedirs(temp_dir_path)  # Recreate for next run
            except OSError as e:
                logger.error(
                    "Error cleaning up temporary directory",
                    extra={
                        "directory": temp_dir_path,
                        "error": str(e),
                    },
                )


def main(config_path: str) -> None:
    """Main entry point for a standard simulation run.

    This function prepares the configuration, sets up logging, and calls
    the main `run_simulation` orchestrator.

    Args:
        config_path (str): The path to the JSON configuration file.
    """
    config, original_config = basic_prepare_config(config_path)
    setup_logging(config, original_config)
    logger.info(
        "Loading configuration",
        extra={
            "config_path": os.path.abspath(config_path),
        },
    )
    try:
        run_simulation(config)
        logger.info("Main execution completed successfully")
    except Exception as e:
        logger.error(
            "Main execution failed", exc_info=True, extra={"exception": str(e)}
        )
        sys.exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run a unified simulation and co-simulation workflow."
    )
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        required=True,
        help="Path to the JSON configuration file.",
    )
    args = parser.parse_args()
    main(args.config)
