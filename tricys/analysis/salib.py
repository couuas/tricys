import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import openai
import pandas as pd
from SALib.analyze import fast, sobol
from SALib.analyze import morris as morris_analyze
from SALib.sample import fast_sampler, latin, morris, saltelli

from tricys.utils.config_utils import get_llm_env

# Configure Chinese fonts in matplotlib
plt.rcParams["font.sans-serif"] = [
    "SimHei",
    "Microsoft YaHei",
    "DejaVu Sans",
    "Arial Unicode MS",
    "sans-serif",
]
plt.rcParams["axes.unicode_minus"] = False

logger = logging.getLogger(__name__)


class TricysSALibAnalyzer:
    """Integrated SALib's Tricys Sensitivity Analyzer.

    Supported Analysis Methods:
    - Sobol: Variance-based global sensitivity analysis
    - Morris: Screening-based sensitivity analysis
    - FAST: Fourier Amplitude Sensitivity Test
    - LHS: Latin Hypercube Sampling uncertainty analysis

    Attributes:
        base_config: Copy of the Tricys base configuration.
        problem: SALib problem definition dictionary.
        parameter_samples: Generated parameter samples array.
        simulation_results: Results from simulations.
        sensitivity_results: Dictionary storing sensitivity analysis results by method.

    Note:
        Automatically sets up Chinese font support and validates Tricys configuration
        on initialization. Supports multiple sensitivity analysis methods with appropriate
        sampling strategies.
    """

    def __init__(self, base_config: Dict[str, Any]) -> None:
        """Initialize the analyzer.

        Args:
            base_config: Tricys base configuration dictionary.

        Note:
            Creates a deep copy of base_config. Initializes problem, samples, and results
            to None. Calls _setup_chinese_font() and _validate_tricys_config() automatically.
        """
        self.base_config = base_config.copy()
        self.problem = None
        self.parameter_samples = None
        self.simulation_results = None
        self.sensitivity_results = {}

        self._setup_chinese_font()
        self._validate_tricys_config()

    def _setup_chinese_font(self) -> None:
        """Set the Chinese font to ensure proper display of Chinese characters in charts.

        Note:
            Tries multiple Chinese fonts in order of preference. Falls back to default
            if no Chinese font found. Also sets axes.unicode_minus to False for proper
            minus sign display. Logs warnings if font setup fails.
        """
        try:
            import matplotlib.font_manager as fm

            chinese_fonts = [
                "SimHei",  # é»‘ä½“
                "Microsoft YaHei",  # å¾®è½¯é›…é»‘
                "KaiTi",  # æ¥·ä½“
                "FangSong",  # ä»¿å®‹
                "STSong",  # åæ–‡å®‹ä½“
                "STKaiti",  # åæ–‡æ¥·ä½“
                "STHeiti",  # åæ–‡é»‘ä½“
                "DejaVu Sans",  # å¤‡ç”¨å­—ä½“
                "Arial Unicode MS",  # å¤‡ç”¨å­—ä½“
            ]

            available_font = None
            system_fonts = [f.name for f in fm.fontManager.ttflist]

            for font in chinese_fonts:
                if font in system_fonts:
                    available_font = font
                    break

            if available_font:
                plt.rcParams["font.sans-serif"] = [available_font] + plt.rcParams[
                    "font.sans-serif"
                ]
                logger.info("Using Chinese font", extra={"font": available_font})
            else:
                logger.warning(
                    "No suitable Chinese font found, which may affect Chinese display"
                )

            plt.rcParams["axes.unicode_minus"] = False

        except Exception as e:
            logger.warning(
                "Failed to set Chinese font, using default font",
                extra={"error": str(e)},
            )

    def _handle_nan_values(
        self, Y: np.ndarray, method_name: str = "Sensitivity analysis"
    ) -> np.ndarray:
        """
        Handling NaN values with maximum interpolation

        Args:
            Y: Output array that may contain NaN values
            method_name: Analysis method name for logging

        Returns:
            Processed output array
        """
        nan_indices = np.isnan(Y)
        if np.any(nan_indices):
            n_nan = np.sum(nan_indices)
            logger.info(
                "Found NaN values, using maximum value for imputation",
                extra={
                    "method_name": method_name,
                    "nan_count": n_nan,
                },
            )

            valid_values = Y[~nan_indices]

            if len(valid_values) > 0:
                max_value = np.max(valid_values)
                Y_processed = Y.copy()
                Y_processed[nan_indices] = max_value
                return Y_processed
            else:
                logger.error(
                    "All values are NaN, analysis cannot be performed",
                    extra={
                        "method_name": method_name,
                    },
                )
                raise ValueError(
                    f"{method_name}: All simulation results are NaN, sensitivity analysis cannot be performed"
                )
        return Y

    def _validate_tricys_config(self) -> None:
        """Validate the Tricys configuration for required sections and keys."""
        required_keys = {
            "paths": ["package_path"],
            "simulation": ["model_name", "stop_time"],
        }

        for section, keys in required_keys.items():
            if section not in self.base_config:
                logger.warning(
                    "Missing configuration section, default values will be used",
                    extra={
                        "section": section,
                    },
                )
                continue

            for key in keys:
                if key not in self.base_config[section]:
                    logger.warning(
                        "Missing configuration item, using default value",
                        extra={
                            "section": section,
                            "key": key,
                        },
                    )

        package_path = self.base_config.get("paths", {}).get("package_path")
        if package_path and not os.path.exists(package_path):
            logger.warning(
                "Model file does not exist, which may cause simulation failure",
                extra={
                    "package_path": package_path,
                },
            )

    def _find_unit_config(self, var_name: str, unit_map: dict) -> dict | None:
        """
        Finds the unit configuration for a variable name from the unit_map.
        1. Checks for an exact match.
        2. Checks if the last part of a dot-separated name matches.
        3. Checks for a simple substring containment as a fallback, matching longest keys first.
        """
        if not unit_map or not var_name:
            return None
        if var_name in unit_map:
            return unit_map[var_name]
        components = var_name.split(".")
        if len(components) > 1 and components[-1] in unit_map:
            return unit_map[components[-1]]
        # Fallback to substring match, longest key first
        for key in sorted(unit_map.keys(), key=len, reverse=True):
            if key in var_name:
                return unit_map[key]
        return None

    def define_problem(
        self,
        param_bounds: Dict[str, Tuple[float, float]],
        param_distributions: Dict[str, str] = None,
    ) -> Dict[str, Any]:
        """Define SALib problem space.

        Args:
            param_bounds: Parameter bounds dictionary {'param_name': (min_val, max_val)}.
            param_distributions: Parameter distribution type dictionary {'param_name': 'unif'/'norm'/etc}.
                Valid distribution types: 'unif', 'triang', 'norm', 'truncnorm', 'lognorm'.

        Returns:
            SALib problem definition dictionary.

        Note:
            Defaults to 'unif' distribution if not specified. Validates distribution types
            and warns if invalid. Logs parameter definitions including bounds and distributions.
        """
        if param_distributions is None:
            param_distributions = {name: "unif" for name in param_bounds.keys()}

        valid_dists = ["unif", "triang", "norm", "truncnorm", "lognorm"]
        for name, dist in param_distributions.items():
            if dist not in valid_dists:
                logger.warning(
                    "Invalid distribution type, using 'unif' instead",
                    extra={
                        "parameter_name": name,
                        "invalid_distribution": dist,
                    },
                )
                param_distributions[name] = "unif"

        self.problem = {
            "num_vars": len(param_bounds),
            "names": list(param_bounds.keys()),
            "bounds": list(param_bounds.values()),
            "dists": [
                param_distributions.get(name, "unif") for name in param_bounds.keys()
            ],
        }

        logger.info(
            "Defined a problem space",
            extra={
                "num_parameters": self.problem["num_vars"],
            },
        )
        for i, name in enumerate(self.problem["names"]):
            logger.info(
                "Parameter definition",
                extra={
                    "parameter_name": name,
                    "bounds": self.problem["bounds"][i],
                    "distribution": self.problem["dists"][i],
                },
            )

        return self.problem

    def generate_samples(
        self, method: str = "sobol", N: int = 1024, **kwargs
    ) -> np.ndarray:
        """Generate parameter samples.

        Args:
            method: Sampling method ('sobol', 'morris', 'fast', 'latin').
            N: Number of samples (for Sobol this is the base sample count, actual count is N*(2*D+2)).
            **kwargs: Method-specific parameters.

        Returns:
            Parameter sample array (n_samples, n_params).

        Raises:
            ValueError: If problem not defined or unsupported method.

        Note:
            Sobol generates N*(2*D+2) samples. Morris generates N trajectories. Samples are
            rounded to 5 decimal places. Stores last sampling method for compatibility checking.
        """
        if self.problem is None:
            raise ValueError(
                "You must first call define_problem() to define the problem space."
            )

        logger.info(
            "Generating samples",
            extra={
                "method": method,
                "base_sample_count": N,
            },
        )

        if method.lower() == "sobol":
            # Sobol method: generate N*(2*D+2) samples
            self.parameter_samples = saltelli.sample(self.problem, N, **kwargs)
            actual_samples = N * (2 * self.problem["num_vars"] + 2)

        elif method.lower() == "morris":
            # Morris method: Generate N trajectories
            # Note: Different versions of SALib may have different parameter names
            morris_kwargs = {"num_levels": 4}
            # Check the SALib version and use the correct parameter names
            try:
                morris_kwargs.update(kwargs)
                self.parameter_samples = morris.sample(self.problem, N, **morris_kwargs)
            except TypeError as e:
                if "grid_jump" in str(e):
                    morris_kwargs = {
                        k: v for k, v in morris_kwargs.items() if k != "grid_jump"
                    }
                    morris_kwargs.update(
                        {k: v for k, v in kwargs.items() if k != "grid_jump"}
                    )
                    self.parameter_samples = morris.sample(
                        self.problem, N, **morris_kwargs
                    )
                else:
                    raise e

            actual_samples = len(self.parameter_samples)

        elif method.lower() == "fast":
            # FAST method
            fast_kwargs = {"M": 4}
            fast_kwargs.update(kwargs)
            self.parameter_samples = fast_sampler.sample(self.problem, N, **fast_kwargs)
            actual_samples = len(self.parameter_samples)

        elif method.lower() == "latin":
            # Latin Hypercube Sampling
            self.parameter_samples = latin.sample(self.problem, N, **kwargs)
            actual_samples = N

        else:
            raise ValueError(f"Unsupported sampling method: {method}")

        logger.info(
            "Successfully generated samples", extra={"actual_samples": actual_samples}
        )

        if self.parameter_samples is not None:
            self.parameter_samples = np.round(self.parameter_samples, decimals=5)
            logger.info("Parameter sample precision adjusted to 5 decimal places")

        self._last_sampling_method = method.lower()

        return self.parameter_samples

    def run_tricys_simulations(self, output_metrics: List[str] = None) -> str:
        """
        Generate sampling parameters and output them as a CSV file, which can be subsequently read by the Tricys simulation engine.

        Args:
            output_metrics: List of output metrics to be extracted (for recording but does not affect CSV generation)
            max_workers: Number of concurrent worker processes (reserved for compatibility, currently unused)

        Returns:
            Path to the generated CSV file
        """
        if self.parameter_samples is None:
            raise ValueError(
                "You must first call generate_samples() to generate samples."
            )

        if output_metrics is None:
            output_metrics = [
                "Startup_Inventory",
                "Self_Sufficiency_Time",
                "Doubling_Time",
            ]

        logger.info("Target output metrics", extra={"output_metrics": output_metrics})

        sampled_param_names = self.problem["names"]

        base_params = self.base_config.get("simulation_parameters", {}).copy()
        csv_output_path = (
            Path(self.base_config.get("paths", {}).get("temp_dir"))
            / "salib_sampling.csv"
        )

        os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)

        param_data = []
        for i, sample in enumerate(self.parameter_samples):
            sampled_params = {
                sampled_param_names[j]: sample[j]
                for j in range(len(sampled_param_names))
            }

            job_params = base_params.copy()
            job_params.update(sampled_params)

            param_data.append(job_params)

        df = pd.DataFrame(param_data)

        for col in df.columns:
            if df[col].dtype in ["float64", "float32"]:
                df[col] = df[col].round(5)

        df.to_csv(csv_output_path, index=False, encoding="utf-8")

        logger.info(
            "Successfully generated parameter samples",
            extra={"num_samples": len(param_data)},
        )
        logger.info("Parameter file saved", extra={"file_path": csv_output_path})
        logger.info("Parameter file columns", extra={"columns": list(df.columns)})
        logger.info("Parameter precision set to 5 decimal places")
        logger.info("Sample statistics", extra={"statistics": df.describe().to_dict()})

        self.sampling_csv_path = csv_output_path

        return csv_output_path

    def generate_tricys_config(
        self, csv_file_path: str = None, output_metrics: List[str] = None
    ) -> Dict[str, Any]:
        """
        Generate Tricys configuration file for reading CSV parameter files and executing simulations
        This function reuses the base configuration and specifically modifies simulation_parameters and analysis_case for file-based SALib runs

        Args:
            csv_file_path: Path to the CSV parameter file. If None, the last generated file is used
            output_metrics: List of output metrics to be calculated

        Returns:
            Path of the generated configuration file
        """
        if csv_file_path is None:
            if hasattr(self, "sampling_csv_path"):
                csv_file_path = self.sampling_csv_path
            else:
                raise ValueError(
                    "CSV file path not found, please first call run_tricys_simulations() or specify csv_file_path"
                )

        if output_metrics is None:
            output_metrics = [
                "Startup_Inventory",
                "Self_Sufficiency_Time",
                "Doubling_Time",
            ]

        csv_abs_path = os.path.abspath(csv_file_path)

        import copy

        tricys_config = copy.deepcopy(self.base_config)
        tricys_config["simulation_parameters"] = {"file": csv_abs_path}

        if "sensitivity_analysis" not in tricys_config:
            tricys_config["sensitivity_analysis"] = {"enabled": True}

        tricys_config["sensitivity_analysis"]["analysis_case"] = {
            "name": "SALib_Analysis",
            "independent_variable": "file",
            "independent_variable_sampling": csv_abs_path,
            "dependent_variables": output_metrics,
        }

        return tricys_config

    def load_tricys_results(
        self, sensitivity_summary_csv: str, output_metrics: List[str] = None
    ) -> np.ndarray:
        """
        Read simulation results from the sensitivity_analysis_summary.csv file output by Tricys

        Args:
            sensitivity_summary_csv: Path to the sensitivity analysis summary CSV file output by Tricys
            output_metrics: List of output metrics to extract

        Returns:
            Simulation result array (n_samples, n_metrics)
        """
        if output_metrics is None:
            output_metrics = [
                "Startup_Inventory",
                "Self_Sufficiency_Time",
                "Doubling_Time",
            ]

        logger.info(f"Read data from the Tricys result file: {sensitivity_summary_csv}")

        df = pd.read_csv(sensitivity_summary_csv)

        logger.info(f"Read {len(df)} simulation results")
        logger.info(f"Result file columns: {list(df.columns)}")

        param_cols = []
        metric_cols = []

        for col in df.columns:
            if col in output_metrics:
                metric_cols.append(col)
            elif col in self.problem["names"] if self.problem else False:
                param_cols.append(col)

        logger.info(f"Recognized parameter columns: {param_cols}")
        logger.info(f"Identified metric columns: {metric_cols}")

        ordered_metric_cols = []
        for metric in output_metrics:
            if metric in metric_cols:
                ordered_metric_cols.append(metric)
            else:
                logger.warning(f"Metric column not found: {metric}")

        if not ordered_metric_cols:
            raise ValueError(f"No valid output metrics columns found: {output_metrics}")

        results_data = df[ordered_metric_cols].values

        self.simulation_results = results_data

        logger.info(f"Successfully loaded simulation results: {results_data.shape}")
        logger.info(
            f"Result Statistics:\n{pd.DataFrame(results_data, columns=ordered_metric_cols).describe()}"
        )
        logger.info(
            f"Result preview:\n{pd.DataFrame(results_data, columns=metric_cols).head()}"
        )
        return self.simulation_results

    def get_compatible_analysis_methods(self, sampling_method: str) -> List[str]:
        """
        Get analysis methods compatible with the specified sampling method.

        Args:
            sampling_method: Sampling method

        Returns:
            List of compatible analysis methods
        """
        compatibility_map = {
            "sobol": ["sobol"],
            "morris": ["morris"],
            "fast": ["fast"],
            "latin": ["latin"],
            "unknown": [],
        }

        return compatibility_map.get(sampling_method, [])

    def run_tricys_analysis(
        self, csv_file_path: str = None, output_metrics: List[str] = None
    ) -> str:
        """
        Run the Tricys simulation using the generated CSV parameter file and obtain the sensitivity analysis results

        Args:
            csv_file_path: Path to the CSV parameter file. If None, the last generated file will be used
            output_metrics: List of output metrics to be calculated
            config_output_path: Path for the configuration file output. If None, it will be automatically generated

        Returns:
            Path to the sensitivity_analysis_summary.csv file
        """
        # Generate Tricys configuration file
        tricys_config = self.generate_tricys_config(
            csv_file_path=csv_file_path, output_metrics=output_metrics
        )

        logger.info("Starting Tricys simulation analysis...")

        try:
            # Call the Tricys simulation engine
            from datetime import datetime

            from tricys.simulation.simulation_analysis import run_simulation

            tricys_config["run_timestamp"] = datetime.now().strftime("%Y%m%d_%H%M%S")

            run_simulation(tricys_config)

            results_dir = tricys_config["paths"]["results_dir"]

            return Path(results_dir) / "sensitivity_analysis_summary.csv"

        except Exception as e:
            logger.error(f"Tricys simulation execution failed: {e}")
            raise

    def analyze_sobol(self, output_index: int = 0, **kwargs) -> Dict[str, Any]:
        """
        Perform Sobol Sensitivity Analysis

        Args:
            output_index: Output variable index
            **kwargs: Sobol analysis parameters

        Returns:
            Sobol sensitivity analysis results

        Note:
            Sobol analysis requires samples generated using the Saltelli sampling method!
            Results from Morris or FAST sampling cannot be used.
        """
        if self.simulation_results is None:
            raise ValueError("The simulation must be run first to obtain the results.")

        # Check sampling method compatibility
        if (
            hasattr(self, "_last_sampling_method")
            and self._last_sampling_method != "sobol"
        ):
            logger.warning(
                f"âš ï¸ Currently using {self._last_sampling_method} sampling, but Sobol analysis requires Saltelli sampling!"
            )
            logger.warning(
                "Suggestion: Regenerate samples using generate_samples('sobol')"
            )

        Y = self.simulation_results[:, output_index]

        Y = self._handle_nan_values(Y, "Sobolåˆ†æ")

        # Remove NaN values
        # valid_indices = ~np.isnan(Y)
        # if not np.all(valid_indices):
        #    logger.warning(f"å‘ç°{np.sum(~valid_indices)}ä¸ªæ— æ•ˆç»“æœï¼Œå°†è¢«æ’é™¤")
        #    Y = Y[valid_indices]
        #    X = self.parameter_samples[valid_indices]
        # else:
        #    X = self.parameter_samples

        try:
            Si = sobol.analyze(self.problem, Y, **kwargs)

            if "sobol" not in self.sensitivity_results:
                self.sensitivity_results["sobol"] = {}

            metric_name = f"metric_{output_index}"
            self.sensitivity_results["sobol"][metric_name] = {
                "output_index": output_index,
                "Si": Si,
                "S1": Si["S1"],
                "ST": Si["ST"],
                "S2": Si.get("S2", None),
                "S1_conf": Si["S1_conf"],
                "ST_conf": Si["ST_conf"],
                "sampling_method": getattr(self, "_last_sampling_method", "unknown"),
            }

            logger.info(f"Sobol sensitivity analysis completed (index {output_index})")
            return self.sensitivity_results["sobol"][metric_name]

        except Exception as e:
            if "saltelli" in str(e).lower() or "sample" in str(e).lower():
                raise ValueError(
                    f"Sobol analysis failed, possibly due to incompatible sampling method: {e}\nPlease regenerate samples using generate_samples('sobol')"
                ) from e
            else:
                raise

    def analyze_morris(self, output_index: int = 0, **kwargs) -> Dict[str, Any]:
        """
        Perform Morris sensitivity analysis

        Args:
            output_index: Output variable index
            **kwargs: Morris analysis parameters

        Returns:
            Morris sensitivity analysis results
        """
        if self.simulation_results is None:
            raise ValueError("The simulation must be run first to obtain the results.")

        Y = self.simulation_results[:, output_index]

        Y = self._handle_nan_values(Y, "Morrisåˆ†æ")
        X = self.parameter_samples

        # Remove NaN values
        # valid_indices = ~np.isnan(Y)
        # if not np.all(valid_indices):
        #    logger.warning(f"å‘ç°{np.sum(~valid_indices)}ä¸ªæ— æ•ˆç»“æœï¼Œå°†è¢«æ’é™¤")
        #    Y = Y[valid_indices]
        #    X = self.parameter_samples[valid_indices]
        # else:
        #    X = self.parameter_samples

        # Perform Morris analysis
        logger.info(
            f"Start Morris sensitivity analysis: X.shape={X.shape}, Y.shape={Y.shape}, X.dtype={X.dtype}"
        )

        try:
            Si = morris_analyze.analyze(self.problem, X, Y, **kwargs)
        except Exception as e:
            logger.error(f"Morris analysis execution failed: {e}")
            logger.error(f"problem: {self.problem}")
            logger.error(f"X shape: {X.shape}, type: {X.dtype}")
            logger.error(f"Yshape: {Y.shape}, type: {Y.dtype}")
            if hasattr(X, "dtype") and X.dtype == "object":
                logger.error(
                    "X contains non-numeric data, please check the sampled data"
                )
            raise

        if "morris" not in self.sensitivity_results:
            self.sensitivity_results["morris"] = {}

        metric_name = f"metric_{output_index}"
        self.sensitivity_results["morris"][metric_name] = {
            "output_index": output_index,
            "Si": Si,
            "mu": Si["mu"],
            "mu_star": Si["mu_star"],
            "sigma": Si["sigma"],
            "mu_star_conf": Si["mu_star_conf"],
        }

        logger.info(f"Morris sensitivity analysis completed (metric {output_index})")
        return self.sensitivity_results["morris"][metric_name]

    def analyze_fast(self, output_index: int = 0, **kwargs) -> Dict[str, Any]:
        """
        Perform FAST sensitivity analysis

        Args:
            output_index: Output variable index
            **kwargs: FAST analysis parameters

        Returns:
            FAST sensitivity analysis results

        Note:
            FAST analysis requires samples generated by the fast_sampler sampling method!
            Results from Morris or Sobol sampling cannot be used.
        """
        if self.simulation_results is None:
            raise ValueError("The simulation must be run first to obtain the results.")

        if (
            hasattr(self, "_last_sampling_method")
            and self._last_sampling_method != "fast"
        ):
            logger.warning(
                f"âš ï¸ The current sampling method is {self._last_sampling_method}, but FAST analysis requires FAST sampling!"
            )
            logger.warning(
                "Suggestion: Regenerate samples using generate_samples('fast')"
            )

        Y = self.simulation_results[:, output_index]

        Y = self._handle_nan_values(Y, "FASTåˆ†æ")

        # Remove NaN values
        # valid_indices = ~np.isnan(Y)
        # if not np.all(valid_indices):
        #    logger.warning(f"å‘ç°{np.sum(~valid_indices)}ä¸ªæ— æ•ˆç»“æœï¼Œå°†è¢«æ’é™¤")
        #    Y = Y[valid_indices]

        try:
            # Perform FAST analysis
            Si = fast.analyze(self.problem, Y, **kwargs)

            if "fast" not in self.sensitivity_results:
                self.sensitivity_results["fast"] = {}

            metric_name = f"metric_{output_index}"
            self.sensitivity_results["fast"][metric_name] = {
                "output_index": output_index,
                "Si": Si,
                "S1": Si["S1"],
                "ST": Si["ST"],
                "sampling_method": getattr(self, "_last_sampling_method", "unknown"),
            }

            logger.info(
                f"FAST sensitivity analysis completed (indicator {output_index})"
            )
            return self.sensitivity_results["fast"][metric_name]

        except Exception as e:
            if "fast" in str(e).lower() or "sample" in str(e).lower():
                raise ValueError(
                    f"FAST analysis failed, possibly due to incompatible sampling method: {e}\nPlease regenerate samples using generate_samples('fast')"
                ) from e
            else:
                raise

    def analyze_lhs(self, output_index: int = 0, **kwargs) -> Dict[str, Any]:
        """
        Perform LHS (Latin Hypercube Sampling) uncertainty analysis

        Note: This is a basic statistical analysis method for LHS samples,
        providing descriptive statistics and basic sensitivity indices.

        Args:
            output_index: Output variable index
            **kwargs: Analysis parameters (reserved for future use)

        Returns:
            LHS uncertainty analysis results
        """
        if self.simulation_results is None:
            raise ValueError("The simulation must be run first to obtain the results.")

        if (
            hasattr(self, "_last_sampling_method")
            and self._last_sampling_method != "latin"
        ):
            logger.warning(
                f"âš ï¸ The current sampling method is {self._last_sampling_method}, but LHS analysis is designed for Latin Hypercube Sampling!"
            )
            logger.warning(
                "Suggestion: Regenerate samples using generate_samples('latin')"
            )

        Y = self.simulation_results[:, output_index]

        # Handle NaN values
        Y = self._handle_nan_values(Y, "LHSåˆ†æ")

        # Basic statistical analysis
        mean_val = np.mean(Y)
        std_val = np.std(Y)
        min_val = np.min(Y)
        max_val = np.max(Y)
        percentile_5 = np.percentile(Y, 5)
        percentile_95 = np.percentile(Y, 95)

        # Create results dictionary
        Si = {
            "mean": mean_val,
            "std": std_val,
            "min": min_val,
            "max": max_val,
            "percentile_5": percentile_5,
            "percentile_95": percentile_95,
        }

        if "latin" not in self.sensitivity_results:
            self.sensitivity_results["latin"] = {}

        metric_name = f"metric_{output_index}"
        self.sensitivity_results["latin"][metric_name] = {
            "output_index": output_index,
            "Si": Si,
            "mean": mean_val,
            "std": std_val,
            "min": min_val,
            "max": max_val,
            "percentile_5": percentile_5,
            "percentile_95": percentile_95,
            "sampling_method": getattr(self, "_last_sampling_method", "unknown"),
        }

        logger.info(f"LHS uncertainty analysis completed (æŒ‡æ ‡ {output_index})")
        return self.sensitivity_results["latin"][metric_name]

    def run_salib_analysis_from_tricys_results(
        self,
        sensitivity_summary_csv: str,
        param_bounds: Dict[str, Tuple[float, float]] = None,
        output_metrics: List[str] = None,
        methods: List[str] = ["sobol", "morris", "fast"],
        save_dir: str = None,
    ) -> Dict[str, Any]:
        """
        Run a complete SALib sensitivity analysis from the sensitivity analysis results file output by Tricys

        Args:
            sensitivity_summary_csv: Path to the sensitivity summary CSV file output by Tricys
            param_bounds: Dictionary of parameter bounds, inferred from the CSV file if None
            output_metrics: List of output metrics to analyze
            methods: List of sensitivity analysis methods to execute
            save_dir: Directory to save the results

        Returns:
            Dictionary containing all analysis results
        """
        if output_metrics is None:
            output_metrics = [
                "Startup_Inventory",
                "Self_Sufficiency_Time",
                "Doubling_Time",
            ]

        if save_dir is None:
            save_dir = os.path.join(
                os.path.dirname(sensitivity_summary_csv), "salib_analysis"
            )
        os.makedirs(save_dir, exist_ok=True)

        df = pd.read_csv(sensitivity_summary_csv)

        if param_bounds is None:
            param_bounds = {}
            param_candidates = []
            for col in df.columns:
                if col not in output_metrics and "." in col:
                    param_candidates.append(col)

            for param in param_candidates:
                param_data = df[param].dropna()
                if len(param_data) > 0:
                    param_bounds[param] = (param_data.min(), param_data.max())

        if not param_bounds:
            raise ValueError(
                "Unable to determine parameter boundaries, please provide the param_bounds parameter"
            )

        self.define_problem(param_bounds)

        self.load_tricys_results(sensitivity_summary_csv, output_metrics)

        detected_method = self._last_sampling_method

        methods = self.get_compatible_analysis_methods(detected_method)

        all_results = {}

        for metric_idx, metric_name in enumerate(output_metrics):
            if metric_idx >= self.simulation_results.shape[1]:
                logger.warning(f"The metric {metric_name} is out of range, skipping")
                continue

            logger.info(f"\n=== Analysis indicators: {metric_name} ===")
            metric_results = {}

            # Check data validity
            Y = self.simulation_results[:, metric_idx]
            valid_ratio = np.sum(~np.isnan(Y)) / len(Y)
            logger.info(f"Valid data ratio: {valid_ratio:.2%}")

            if valid_ratio < 0.5:
                logger.warning(
                    f"The metric {metric_name} has less than 50% valid data, which may affect the analysis quality."
                )

            # Sobol analysis
            if "sobol" in methods:
                try:
                    logger.info("Performing Sobol sensitivity analysis...")
                    sobol_result = self.analyze_sobol(output_index=metric_idx)
                    metric_results["sobol"] = sobol_result

                    # Display Sobol results summary
                    logger.info("\nSobol sensitivity index:")
                    for i, param_name in enumerate(self.problem["names"]):
                        s1 = sobol_result["S1"][i]
                        st = sobol_result["ST"][i]
                        logger.info(f"  {param_name}: S1={s1:.4f}, ST={st:.4f}")

                except Exception as e:
                    logger.error(f"Sobol analysis failed: {e}")

            # Morris analysis
            if "morris" in methods:
                try:
                    logger.info("Performing Morris sensitivity analysis...")
                    morris_result = self.analyze_morris(output_index=metric_idx)
                    metric_results["morris"] = morris_result

                    # Display Morris results summary
                    logger.info("\nMorris sensitivity index:")
                    for i, param_name in enumerate(self.problem["names"]):
                        mu_star = morris_result["mu_star"][i]
                        sigma = morris_result["sigma"][i]
                        logger.info(f"  {param_name}: Î¼*={mu_star:.4f}, Ïƒ={sigma:.4f}")

                except Exception as e:
                    logger.error(f"Morris analysis failed: {e}")

            # FAST analysis
            if "fast" in methods:
                try:
                    logger.info("Performing FAST sensitivity analysis...")
                    fast_result = self.analyze_fast(output_index=metric_idx)
                    metric_results["fast"] = fast_result

                    # Display FAST results summary
                    logger.info("\nFAST sensitivity index:")
                    for i, param_name in enumerate(self.problem["names"]):
                        s1 = fast_result["S1"][i]
                        st = fast_result["ST"][i]
                        logger.info(f"  {param_name}: S1={s1:.4f}, ST={st:.4f}")

                except Exception as e:
                    logger.error(f"FAST analysis failed: {e}")

            # LHS analysis
            if "latin" in methods:
                try:
                    logger.info("Performing LHS uncertainty analysis...")
                    lhs_result = self.analyze_lhs(output_index=metric_idx)
                    metric_results["latin"] = lhs_result

                    # Display LHS results summary
                    logger.info("\nLHSåˆ†æç»“æœ:")
                    logger.info(f"  å‡å€¼: {lhs_result['mean']:.4f}")
                    logger.info(f"  æ ‡å‡†å·®: {lhs_result['std']:.4f}")
                    logger.info(f"  æœ€å°å€¼: {lhs_result['min']:.4f}")
                    logger.info(f"  æœ€å¤§å€¼: {lhs_result['max']:.4f}")
                    logger.info(f"  5%åˆ†ä½æ•°: {lhs_result['percentile_5']:.4f}")
                    logger.info(f"  95%åˆ†ä½æ•°: {lhs_result['percentile_95']:.4f}")

                except Exception as e:
                    logger.error(f"LHSåˆ†æå¤±è´¥: {e}")

            all_results[metric_name] = metric_results

        try:
            if "sobol" in methods and "sobol" in self.sensitivity_results:
                self.plot_sobol_results(save_dir=save_dir, metric_names=output_metrics)

            if "morris" in methods and "morris" in self.sensitivity_results:
                self.plot_morris_results(save_dir=save_dir, metric_names=output_metrics)

            if "fast" in methods and "fast" in self.sensitivity_results:
                self.plot_fast_results(save_dir=save_dir, metric_names=output_metrics)

            # Plot LHS results
            if "latin" in methods and "latin" in self.sensitivity_results:
                self.plot_lhs_results(save_dir=save_dir, metric_names=output_metrics)

        except Exception as e:
            logger.warning(f"Drawing failed: {e}")

        try:
            self.save_results(
                save_dir=save_dir, format="csv", metric_names=output_metrics
            )

            report_content = self._save_sensitivity_report(all_results, save_dir)
            report_path = os.path.join(save_dir, "analysis_report.md")

            env = get_llm_env(self.base_config)

            # --- LLM Calls for analysis ---
            api_key = env.get("API_KEY")
            base_url = env.get("BASE_URL")
            ai_model = env.get("AI_MODEL")

            sa_config = self.base_config.get("sensitivity_analysis", {})
            case_config = sa_config.get("analysis_case", {})
            ai_config = case_config.get("ai")

            ai_enabled = False
            if isinstance(ai_config, bool):
                ai_enabled = ai_config
            elif isinstance(ai_config, dict):
                ai_enabled = ai_config.get("enabled", False)

            if api_key and base_url and ai_model and ai_enabled:
                # First LLM call for initial analysis
                wrapper_prompt, llm_summary = call_llm_for_salib_analysis(
                    report_content=report_content,
                    api_key=api_key,
                    base_url=base_url,
                    ai_model=ai_model,
                    method=detected_method,
                )
                if wrapper_prompt and llm_summary:
                    with open(report_path, "a", encoding="utf-8") as f:
                        f.write("\n\n---\n\n# AIæ¨¡å‹åˆ†ææç¤ºè¯\n\n")
                        f.write("```markdown\n")
                        f.write(wrapper_prompt)
                        f.write("\n```\n\n")
                        f.write("\n\n---\n\n# AIæ¨¡å‹åˆ†æç»“æœ\n\n")
                        f.write(llm_summary)
                    logger.info(f"Appended LLM prompt and summary to {report_path}")

                    # Second LLM call for academic report
                    glossary_path = None
                    if isinstance(case_config, dict):
                        glossary_path = sa_config.get("glossary_path")

                    if glossary_path and os.path.exists(glossary_path):
                        try:
                            with open(glossary_path, "r", encoding="utf-8") as f:
                                glossary_content = f.read()

                            (
                                academic_wrapper_prompt,
                                academic_report,
                            ) = call_llm_for_academic_report(
                                analysis_report=llm_summary,
                                glossary_content=glossary_content,
                                api_key=api_key,
                                base_url=base_url,
                                ai_model=ai_model,
                                problem_details=self.problem,
                                metric_names=output_metrics,
                                method=detected_method,
                                save_dir=save_dir,
                            )

                            if academic_wrapper_prompt and academic_report:
                                academic_report_path = os.path.join(
                                    save_dir, "academic_report.md"
                                )
                                with open(
                                    academic_report_path, "w", encoding="utf-8"
                                ) as f:
                                    f.write(academic_report)
                                logger.info(
                                    f"Generated academic report: {academic_report_path}"
                                )
                        except Exception as e:
                            logger.error(
                                f"Failed to generate or save academic report: {e}"
                            )
                    elif glossary_path:
                        logger.warning(
                            f"Glossary file not found at {glossary_path}, skipping academic report generation."
                        )

            else:
                logger.warning(
                    "API_KEY, BASE_URL, or AI_MODEL not set, or AI analysis is disabled. Skipping LLM summary generation."
                )

        except Exception as e:
            logger.warning(f"Failed to save result: {e}")

        logger.info("\nâœ… SALib sensitivity analysis completed!")
        logger.info(f"ğŸ“ The result has been saved to: {save_dir}")

        return all_results

    def _save_sensitivity_report(
        self, all_results: Dict[str, Any], save_dir: str
    ) -> str:
        """The result has been saved to: {save_dir}"""
        report_file = os.path.join(save_dir, "analysis_report.md")
        # Determine analysis type based on sampling method
        is_uncertainty_analysis = (
            hasattr(self, "_last_sampling_method")
            and self._last_sampling_method == "latin"
        )
        report_title = (
            "# SALib ä¸ç¡®å®šæ€§åˆ†ææŠ¥å‘Š\n\n"
            if is_uncertainty_analysis
            else "# SALib æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š\n\n"
        )
        report_lines = []
        report_lines.append(report_title)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n")
        # Get unit_map from config
        sensitivity_analysis_config = self.base_config.get("sensitivity_analysis", {})
        unit_map = sensitivity_analysis_config.get("unit_map", {})
        report_lines.append("## åˆ†æå‚æ•°\n\n")
        if self.problem:
            for i, param_name in enumerate(self.problem["names"]):
                bounds = self.problem["bounds"][i]
                # --- Unit Conversion Logic for Bounds ---
                unit_config = self._find_unit_config(param_name, unit_map)
                display_bounds = list(bounds)
                unit_str = ""
                if unit_config:
                    unit = unit_config.get("unit")
                    factor = unit_config.get("conversion_factor")
                    if factor:
                        display_bounds[0] /= float(factor)
                        display_bounds[1] /= float(factor)
                    if unit:
                        unit_str = f" ({unit})"
                # --- End Conversion Logic ---
                report_lines.append(
                    f"- **{param_name}**: [{display_bounds[0]:.4f}, {display_bounds[1]:.4f}]{unit_str}\n"
                )
        report_lines.append("\n")
        for metric_name, metric_results in all_results.items():
            metric_section_title = (
                f"## {metric_name} ä¸ç¡®å®šæ€§åˆ†æç»“æœ\n\n"
                if is_uncertainty_analysis
                else f"## {metric_name} æ•æ„Ÿæ€§åˆ†æç»“æœ\n\n"
            )
            report_lines.append(metric_section_title)
            if "sobol" in metric_results:
                report_lines.append("### Sobolæ•æ„Ÿæ€§æŒ‡æ•°\n\n")
                report_lines.append(
                    "| å‚æ•° | S1 (ä¸€é˜¶) | ST (æ€») | S1ç½®ä¿¡åŒºé—´ | STç½®ä¿¡åŒºé—´ |\n"
                )
                report_lines.append(
                    "|------|----------|---------|------------|------------|\n"
                )
                sobol_data = metric_results["sobol"]
                for i, param_name in enumerate(self.problem["names"]):
                    s1 = sobol_data["S1"][i]
                    st = sobol_data["ST"][i]
                    s1_conf = sobol_data["S1_conf"][i]
                    st_conf = sobol_data["ST_conf"][i]
                    report_lines.append(
                        f"| {param_name} | {s1:.4f} | {st:.4f} | Â±{s1_conf:.4f} | Â±{st_conf:.4f} |\n"
                    )
                report_lines.append("\n")
                plot_filename = (
                    f'sobol_sensitivity_indices_{metric_name.replace(" ", "_")}.png'
                )
                report_lines.append(
                    f"![Sobol Analysis for {metric_name}]({plot_filename})\n\n"
                )
            if "morris" in metric_results:
                report_lines.append("### Morrisæ•æ„Ÿæ€§æŒ‡æ•°\n\n")
                report_lines.append(
                    "| å‚æ•° | Î¼* (å¹³å‡ç»å¯¹æ•ˆåº”) | Ïƒ (æ ‡å‡†å·®) | Î¼*ç½®ä¿¡åŒºé—´ |\n"
                )
                report_lines.append(
                    "|------|-------------------|------------|------------|\n"
                )
                morris_data = metric_results["morris"]
                for i, param_name in enumerate(self.problem["names"]):
                    mu_star = morris_data["mu_star"][i]
                    sigma = morris_data["sigma"][i]
                    mu_star_conf = morris_data["mu_star_conf"][i]
                    report_lines.append(
                        f"| {param_name} | {mu_star:.4f} | {sigma:.4f} | Â±{mu_star_conf:.4f} |\n"
                    )
                report_lines.append("\n")
                plot_filename = (
                    f'morris_sensitivity_analysis_{metric_name.replace(" ", "_")}.png'
                )
                report_lines.append(
                    f"![Morris Analysis for {metric_name}]({plot_filename})\n\n"
                )
            if "fast" in metric_results:
                report_lines.append("### FASTæ•æ„Ÿæ€§æŒ‡æ•°\n\n")
                report_lines.append("| å‚æ•° | S1 (ä¸€é˜¶) | ST (æ€») |\n")
                report_lines.append("|------|----------|---------|\n")
                fast_data = metric_results["fast"]
                for i, param_name in enumerate(self.problem["names"]):
                    s1 = fast_data["S1"][i]
                    st = fast_data["ST"][i]
                    report_lines.append(f"| {param_name} | {s1:.4f} | {st:.4f} |\n")
                report_lines.append("\n")
                plot_filename = (
                    f'fast_sensitivity_indices_{metric_name.replace(" ", "_")}.png'
                )
                report_lines.append(
                    f"![FAST Analysis for {metric_name}]({plot_filename})\n\n"
                )
            if "latin" in metric_results:
                # --- Unit Conversion Logic for Metrics ---
                unit_config = self._find_unit_config(metric_name, unit_map)
                unit_str = ""
                factor = 1.0
                if unit_config:
                    unit = unit_config.get("unit")
                    conv_factor = unit_config.get("conversion_factor")
                    if conv_factor:
                        factor = float(conv_factor)
                    if unit:
                        unit_str = f" ({unit})"
                # --- End Conversion Logic ---
                # 1. Get raw data and clean it
                output_index = metric_results["latin"]["output_index"]
                Y = self.simulation_results[:, output_index]
                Y_clean = Y[~np.isnan(Y)]
                # --- Modify report generation ---
                report_lines.append("### ç»Ÿè®¡æ‘˜è¦\n\n")
                lhs_data = metric_results["latin"]
                report_lines.append(
                    f"- å‡å€¼: {lhs_data['mean']/factor:.4f}{unit_str}\n"
                )
                report_lines.append(
                    f"- æ ‡å‡†å·®: {lhs_data['std']/factor:.4f}{unit_str}\n"
                )
                report_lines.append(
                    f"- æœ€å°å€¼: {lhs_data['min']/factor:.4f}{unit_str}\n"
                )
                report_lines.append(
                    f"- æœ€å¤§å€¼: {lhs_data['max']/factor:.4f}{unit_str}\n\n"
                )
                # 2. Calculate more percentiles
                if len(Y_clean) > 0:
                    percentiles_to_calc = [5, 10, 25, 50, 75, 90, 95]
                    percentile_values = np.percentile(Y_clean, percentiles_to_calc)
                    report_lines.append("### åˆ†å¸ƒå…³é”®ç‚¹ (CDF)\n\n")
                    report_lines.append(
                        f"- 5%åˆ†ä½æ•°: {percentile_values[0]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 10%åˆ†ä½æ•°: {percentile_values[1]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 25%åˆ†ä½æ•° (Q1): {percentile_values[2]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 50%åˆ†ä½æ•° (ä¸­ä½æ•°): {percentile_values[3]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 75%åˆ†ä½æ•° (Q3): {percentile_values[4]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 90%åˆ†ä½æ•°: {percentile_values[5]/factor:.4f}{unit_str}\n"
                    )
                    report_lines.append(
                        f"- 95%åˆ†ä½æ•°: {percentile_values[6]/factor:.4f}{unit_str}\n\n"
                    )
                    # 3. Calculate histogram data
                    hist_freq, bin_edges = np.histogram(Y_clean, bins=10)
                    report_lines.append("### è¾“å‡ºåˆ†å¸ƒ (ç›´æ–¹å›¾æ•°æ®)\n\n")
                    report_lines.append("| æ•°å€¼èŒƒå›´ | é¢‘æ•° |\n")
                    report_lines.append("|:---|---:|\n")
                    for i in range(len(hist_freq)):
                        lower_bound = bin_edges[i] / factor
                        upper_bound = bin_edges[i + 1] / factor
                        freq = hist_freq[i]
                        report_lines.append(
                            f"| {lower_bound:.2f} - {upper_bound:.2f} | {freq} |\n"
                        )
                    report_lines.append("\n")
                plot_filename = f'lhs_analysis_{metric_name.replace(" ", "_")}.png'
                report_lines.append(
                    f"![LHS Analysis for {metric_name}]({plot_filename})\n\n"
                )
        report_content = "".join(report_lines)
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_content)
        logger.info(f"The sensitivity analysis report has been saved.: {report_file}")
        return report_content

    def plot_sobol_results(
        self,
        save_dir: str = None,
        figsize: Tuple[int, int] = (12, 8),
        metric_names: List[str] = None,
    ) -> None:
        """Plot Sobol analysis results"""
        if "sobol" not in self.sensitivity_results:
            raise ValueError("No analysis results for the Sobol method were found.")

        # Ensure Chinese font settings
        self._setup_chinese_font()

        if save_dir is None:
            save_dir = "."
        os.makedirs(save_dir, exist_ok=True)

        # Get the results of all indicators
        sobol_results = self.sensitivity_results["sobol"]

        if not sobol_results:
            raise ValueError("Sobol analysis results not found")

        # Generate charts for each metric
        for metric_key, results in sobol_results.items():
            Si = results["Si"]
            output_index = results["output_index"]

            if metric_names and output_index < len(metric_names):
                metric_display_name = metric_names[output_index]
            else:
                metric_display_name = f"Metric_{output_index}"

            # Bar chart of first-order and total sensitivity indices
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

            # First-order sensitivity index
            y_pos = np.arange(len(self.problem["names"]))
            ax1.barh(y_pos, Si["S1"], xerr=Si["S1_conf"], alpha=0.7, color="skyblue")
            ax1.set_yticks(y_pos)
            ax1.set_yticklabels(self.problem["names"], fontsize=10)
            ax1.set_xlabel("First-order sensitivity index (S1)", fontsize=12)
            ax1.set_title(
                f"First-order Sensitivity Indices\n{metric_display_name}",
                fontsize=14,
                pad=20,
            )
            ax1.grid(True, alpha=0.3)

            # # Total Sensitivity Index
            ax2.barh(y_pos, Si["ST"], xerr=Si["ST_conf"], alpha=0.7, color="orange")
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(self.problem["names"], fontsize=10)
            ax2.set_xlabel("Total Sensitivity Index (ST)", fontsize=12)
            ax2.set_title(
                f"Total Sensitivity Indices\n{metric_display_name}", fontsize=14, pad=20
            )
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            filename = (
                f'sobol_sensitivity_indices_{metric_display_name.replace(" ", "_")}.png'
            )
            plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches="tight")

            logger.info(f"Sobol result chart has been saved: {filename}")

    def plot_morris_results(
        self,
        save_dir: str = None,
        figsize: Tuple[int, int] = (12, 8),
        metric_names: List[str] = None,
    ) -> None:
        """Plot the Morris analysis results"""
        if "morris" not in self.sensitivity_results:
            raise ValueError("No analysis results were found for the Morris method.")

        # Ensure Chinese font settings
        self._setup_chinese_font()

        if save_dir is None:
            save_dir = "."
        os.makedirs(save_dir, exist_ok=True)

        # Obtain the results of all indicators
        morris_results = self.sensitivity_results["morris"]

        if not morris_results:
            raise ValueError("No Morris analysis results found")

        for metric_key, results in morris_results.items():
            Si = results["Si"]
            output_index = results["output_index"]

            if metric_names and output_index < len(metric_names):
                metric_display_name = metric_names[output_index]
            else:
                metric_display_name = f"Metric_{output_index}"

            # Morris Î¼*-Ïƒ diagram
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

            # Î¼*-Ïƒ scatter plot
            ax1.scatter(Si["mu_star"], Si["sigma"], s=100, alpha=0.7, color="red")
            for i, name in enumerate(self.problem["names"]):
                ax1.annotate(
                    name,
                    (Si["mu_star"][i], Si["sigma"][i]),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=9,
                )

            ax1.set_xlabel("Î¼*(Average Absolute Effect)", fontsize=12)
            ax1.set_ylabel("Ïƒ (Standard Deviation)", fontsize=12)
            ax1.set_title(
                f"Morris Î¼*-Ïƒ Plot\n{metric_display_name}", fontsize=14, pad=20
            )
            ax1.grid(True, alpha=0.3)

            y_pos = np.arange(len(self.problem["names"]))
            ax2.barh(
                y_pos, Si["mu_star"], xerr=Si["mu_star_conf"], alpha=0.7, color="green"
            )
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(self.problem["names"], fontsize=10)
            ax2.set_xlabel("Î¼*(Average Absolute Effect)", fontsize=12)
            ax2.set_title(
                f"Morris Elementary Effects\n{metric_display_name}", fontsize=14, pad=20
            )
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            filename = f'morris_sensitivity_analysis_{metric_display_name.replace(" ", "_")}.png'
            plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches="tight")

            logger.info(f"Morris result chart has been saved: {filename}")

    def plot_fast_results(
        self,
        save_dir: str = None,
        figsize: Tuple[int, int] = (12, 8),
        metric_names: List[str] = None,
    ) -> None:
        """Plot FAST analysis results"""
        if "fast" not in self.sensitivity_results:
            raise ValueError("No analysis results found for the FAST method")

        # No analysis results found for the FAST method
        self._setup_chinese_font()

        if save_dir is None:
            save_dir = "."
        os.makedirs(save_dir, exist_ok=True)

        # Get the results of all indicators
        fast_results = self.sensitivity_results["fast"]

        if not fast_results:
            raise ValueError("FAST analysis results not found")

        # Generate a chart for each metric
        for metric_key, results in fast_results.items():
            Si = results["Si"]
            output_index = results["output_index"]

            # Determine the indicator name
            if metric_names and output_index < len(metric_names):
                metric_display_name = metric_names[output_index]
            else:
                metric_display_name = f"Metric_{output_index}"

            # Bar charts of first-order and total sensitivity indices
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

            # first-order sensitivity index
            y_pos = np.arange(len(self.problem["names"]))
            ax1.barh(y_pos, Si["S1"], alpha=0.7, color="purple")
            ax1.set_yticks(y_pos)
            ax1.set_yticklabels(self.problem["names"], fontsize=10)
            ax1.set_xlabel("ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•° (S1)", fontsize=12)
            ax1.set_title(
                f"FAST First-order Sensitivity Indices\n{metric_display_name}",
                fontsize=14,
                pad=20,
            )
            ax1.grid(True, alpha=0.3)

            # Total Sensitivity Index
            ax2.barh(y_pos, Si["ST"], alpha=0.7, color="darkgreen")
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(self.problem["names"], fontsize=10)
            ax2.set_xlabel("æ€»æ•æ„Ÿæ€§æŒ‡æ•° (ST)", fontsize=12)
            ax2.set_title(
                f"FAST Total Sensitivity Indices\n{metric_display_name}",
                fontsize=14,
                pad=20,
            )
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            filename = (
                f'fast_sensitivity_indices_{metric_display_name.replace(" ", "_")}.png'
            )
            plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches="tight")

            logger.info(f"The FAST result chart has been saved: {filename}")

    def plot_lhs_results(
        self,
        save_dir: str = None,
        figsize: Tuple[int, int] = (12, 8),
        metric_names: List[str] = None,
    ) -> None:
        """Plot LHS (Latin Hypercube Sampling) uncertainty analysis results"""
        if "latin" not in self.sensitivity_results:
            raise ValueError("No analysis results found for the LHS method")

        # Ensure Chinese font settings
        self._setup_chinese_font()

        if save_dir is None:
            save_dir = "."
        os.makedirs(save_dir, exist_ok=True)

        # Get the results of all indicators
        lhs_results = self.sensitivity_results["latin"]

        if not lhs_results:
            raise ValueError("LHS analysis results not found")

        # Generate charts for each metric
        for metric_key, results in lhs_results.items():
            Si = results["Si"]
            output_index = results["output_index"]

            # Determine the indicator name
            if metric_names and output_index < len(metric_names):
                metric_display_name = metric_names[output_index]
            else:
                metric_display_name = f"Metric_{output_index}"

            # Get unit from config
            sensitivity_analysis_config = self.base_config.get(
                "sensitivity_analysis", {}
            )
            unit_map = sensitivity_analysis_config.get("unit_map", {})
            unit_config = self._find_unit_config(metric_display_name, unit_map)
            unit_str = ""
            if unit_config:
                unit = unit_config.get("unit")
                if unit:
                    unit_str = f" ({unit})"

            xlabel = f"{metric_display_name}{unit_str}"

            # Create a figure with two subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

            # Plot 1: Distribution histogram
            ax1.hist(
                self.simulation_results[:, output_index],
                bins=30,
                alpha=0.7,
                color="skyblue",
                edgecolor="black",
            )
            ax1.set_xlabel(xlabel, fontsize=12)
            ax1.set_ylabel("é¢‘ç‡", fontsize=12)
            ax1.set_title("è¾“å‡ºåˆ†å¸ƒç›´æ–¹å›¾", fontsize=14, pad=10)
            ax1.grid(True, alpha=0.3)

            # Add statistics text to the histogram plot
            stats_text = f"å‡å€¼: {Si['mean']:.4f}\næ ‡å‡†å·®: {Si['std']:.4f}\næœ€å°å€¼: {Si['min']:.4f}\næœ€å¤§å€¼: {Si['max']:.4f}"
            ax1.text(
                0.05,
                0.95,
                stats_text,
                transform=ax1.transAxes,
                fontsize=10,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
            )

            # Plot 2: Cumulative distribution function
            sorted_data = np.sort(self.simulation_results[:, output_index])
            y_vals = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
            ax2.plot(sorted_data, y_vals, linewidth=2, color="darkgreen")
            ax2.set_xlabel(xlabel, fontsize=12)
            ax2.set_ylabel("ç´¯ç§¯æ¦‚ç‡", fontsize=12)
            ax2.set_title("ç´¯ç§¯åˆ†å¸ƒå‡½æ•°", fontsize=14, pad=10)
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            filename = f'lhs_analysis_{metric_display_name.replace(" ", "_")}.png'
            plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches="tight")

            logger.info(f"LHSåˆ†æç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")

    def save_results(
        self, save_dir: str = None, format: str = "csv", metric_names: List[str] = None
    ) -> None:
        """
        Save sensitivity analysis results

        Args:
            save_dir: Save directory
            format: Save format ('csv
        """
        if save_dir is None:
            save_dir = "."
        os.makedirs(save_dir, exist_ok=True)

        for method, method_results in self.sensitivity_results.items():
            if not method_results:
                continue

            for metric_key, results in method_results.items():
                output_index = results["output_index"]

                if metric_names and output_index < len(metric_names):
                    metric_display_name = metric_names[output_index]
                else:
                    metric_display_name = f"Metric_{output_index}"

                if format == "csv":
                    if method == "sobol":
                        sobol_df = pd.DataFrame(
                            {
                                "Parameter": self.problem["names"],
                                "S1": results["S1"],
                                "ST": results["ST"],
                                "S1_conf": results["S1_conf"],
                                "ST_conf": results["ST_conf"],
                            }
                        )
                        filename = (
                            f'sobol_indices_{metric_display_name.replace(" ", "_")}.csv'
                        )
                        sobol_df.to_csv(os.path.join(save_dir, filename), index=False)
                        logger.info(f"Sobol results have been saved: {filename}")

                    elif method == "morris":
                        morris_df = pd.DataFrame(
                            {
                                "Parameter": self.problem["names"],
                                "mu": results["mu"],
                                "mu_star": results["mu_star"],
                                "sigma": results["sigma"],
                                "mu_star_conf": results["mu_star_conf"],
                            }
                        )
                        filename = f'morris_indices_{metric_display_name.replace(" ", "_")}.csv'
                        morris_df.to_csv(os.path.join(save_dir, filename), index=False)
                        logger.info(f"Morris results have been saved: {filename}")

                    elif method == "fast":
                        fast_df = pd.DataFrame(
                            {
                                "Parameter": self.problem["names"],
                                "S1": results["S1"],
                                "ST": results["ST"],
                            }
                        )
                        filename = (
                            f'fast_indices_{metric_display_name.replace(" ", "_")}.csv'
                        )
                        fast_df.to_csv(os.path.join(save_dir, filename), index=False)
                        logger.info(f"FAST results have been saved: {filename}")

                    elif method == "latin":
                        # Save LHS statistics
                        lhs_stats_df = pd.DataFrame(
                            {
                                "Metric": [metric_display_name],
                                "Mean": [results["mean"]],
                                "Std": [results["std"]],
                                "Min": [results["min"]],
                                "Max": [results["max"]],
                                "Percentile_5": [results["percentile_5"]],
                                "Percentile_95": [results["percentile_95"]],
                            }
                        )
                        filename_stats = (
                            f'lhs_stats_{metric_display_name.replace(" ", "_")}.csv'
                        )
                        lhs_stats_df.to_csv(
                            os.path.join(save_dir, filename_stats), index=False
                        )
                        logger.info(f"LHSç»Ÿè®¡ç»“æœå·²ä¿å­˜: {filename_stats}")

                        # Remove LHS sensitivity indices saving
                        # lhs_sens_df = pd.DataFrame({
                        #     "Parameter": self.problem["names"],
                        #     "Partial_Correlation": results["partial_correlations"]
                        # })
                        # filename_sens = f'lhs_sensitivity_{metric_display_name.replace(" ", "_")}.csv'
                        # lhs_sens_df.to_csv(os.path.join(save_dir, filename_sens), index=False)
                        # logger.info(f"LHSæ•æ„Ÿæ€§ç»“æœå·²ä¿å­˜: {filename_sens}")

        logger.info(f"The result has been saved to: {save_dir}")


def call_llm_for_salib_analysis(
    report_content: str, api_key: str, base_url: str, ai_model: str, method: str
) -> Tuple[str, str]:
    """Sends a SALib analysis report to an LLM for summarization and returns the prompt and summary."""
    try:
        logger.info("Proceeding with LLM analysis for SALib report.")

        PROMPT_TEMPLATES = {
            "sobol": """**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ååœ¨æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸå…·æœ‰æ·±åšèƒŒæ™¯çš„æ•æ„Ÿæ€§åˆ†æä¸“å®¶ã€‚

**ä»»åŠ¡ï¼š** è¯·ä»”ç»†å®¡æŸ¥å¹¶è§£è¯»ä»¥ä¸‹è¿™ä»½ç”±SALibåº“ç”Ÿæˆçš„**Sobolæ•æ„Ÿæ€§åˆ†æ**æŠ¥å‘Šã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1.  **æ€»ç»“æ ¸å¿ƒå‘ç°**ï¼šå¯¹äºæŠ¥å‘Šä¸­æåˆ°çš„æ¯ä¸€ä¸ªè¾“å‡ºæŒ‡æ ‡ï¼ˆå¦‚â€œå¯åŠ¨æ°šé‡â€ç­‰ï¼‰ï¼Œæ€»ç»“å…¶æ•æ„Ÿæ€§åˆ†æç»“æœã€‚
2.  **è¯†åˆ«å…³é”®å‚æ•°**ï¼šæ˜ç¡®æŒ‡å‡ºå“ªäº›è¾“å…¥å‚æ•°çš„**ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆS1ï¼‰**å’Œ**æ€»æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆSTï¼‰**æœ€é«˜ã€‚
3.  **è§£è¯»æŒ‡æ•°å«ä¹‰**ï¼šè§£é‡ŠS1å’ŒSTæŒ‡æ•°çš„å«ä¹‰ã€‚ä¾‹å¦‚ï¼Œé«˜S1å€¼è¡¨ç¤ºå‚æ•°å¯¹è¾“å‡ºæœ‰é‡è¦çš„ç›´æ¥å½±å“ï¼Œè€ŒSTä¸S1çš„æ˜¾è‘—å·®å¼‚è¡¨ç¤ºå‚æ•°å­˜åœ¨å¼ºçƒˆçš„äº¤äº’ä½œç”¨æˆ–éçº¿æ€§æ•ˆåº”ã€‚
4.  **æä¾›ç»¼åˆç»“è®º**ï¼šåŸºäºæ‰€æœ‰åˆ†æç»“æœï¼Œå¯¹æ¨¡å‹çš„æ•´ä½“è¡Œä¸ºã€å‚æ•°é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥åŠè¿™äº›å‘ç°å¯¹å·¥ç¨‹å®è·µçš„æ½œåœ¨å¯ç¤ºï¼Œç»™å‡ºä¸€ä¸ªç»¼åˆæ€§çš„ç»“è®ºã€‚

è¯·ç¡®ä¿ä½ çš„åˆ†ææ¸…æ™°ã€ä¸“ä¸šï¼Œå¹¶ç›´æ¥åˆ‡å…¥è¦ç‚¹ã€‚
""",
            "morris": """**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ååœ¨æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸå…·æœ‰æ·±åšèƒŒæ™¯çš„æ•æ„Ÿæ€§åˆ†æä¸“å®¶ã€‚

**ä»»åŠ¡ï¼š** è¯·ä»”ç»†å®¡æŸ¥å¹¶è§£è¯»ä»¥ä¸‹è¿™ä»½ç”±SALibåº“ç”Ÿæˆçš„**Morrisæ•æ„Ÿæ€§åˆ†æ**æŠ¥å‘Šã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1.  **æ€»ç»“æ ¸å¿ƒå‘ç°**ï¼šå¯¹äºæŠ¥å‘Šä¸­æåˆ°çš„æ¯ä¸€ä¸ªè¾“å‡ºæŒ‡æ ‡ï¼ˆå¦‚â€œå¯åŠ¨æ°šé‡â€ç­‰ï¼‰ï¼Œæ€»ç»“å…¶æ•æ„Ÿæ€§åˆ†æç»“æœã€‚
2.  **è¯†åˆ«å…³é”®å‚æ•°**ï¼šæ ¹æ®**Î¼* (mu_star)**å€¼å¯¹å‚æ•°è¿›è¡Œæ’åºï¼Œè¯†åˆ«å‡ºå¯¹æ¨¡å‹è¾“å‡ºå½±å“æœ€å¤§çš„å‚æ•°ã€‚
3.  **è§£è¯»å‚æ•°æ•ˆåº”**ï¼šè§£é‡Š**Î¼***å’Œ**Ïƒ (sigma)**çš„å«ä¹‰ã€‚é«˜Î¼*è¡¨ç¤ºå‚æ•°æœ‰é‡è¦å½±å“ï¼Œé«˜Ïƒè¡¨ç¤ºå‚æ•°å­˜åœ¨éçº¿æ€§å½±å“æˆ–ä¸å…¶ä»–å‚æ•°æœ‰äº¤äº’ä½œç”¨ã€‚ç»“åˆÎ¼*-Ïƒå›¾è¿›è¡Œåˆ†æã€‚
4.  **æä¾›ç»¼åˆç»“è®º**ï¼šåŸºäºæ‰€æœ‰åˆ†æç»“æœï¼Œå¯¹æ¨¡å‹çš„æ•´ä½“è¡Œä¸ºã€å‚æ•°é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥åŠè¿™äº›å‘ç°å¯¹å·¥ç¨‹å®è·µçš„æ½œåœ¨å¯ç¤ºï¼Œç»™å‡ºä¸€ä¸ªç»¼åˆæ€§çš„ç»“è®ºã€‚

è¯·ç¡®ä¿ä½ çš„åˆ†ææ¸…æ™°ã€ä¸“ä¸šï¼Œå¹¶ç›´æ¥åˆ‡å…¥è¦ç‚¹ã€‚
""",
            "fast": """**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ååœ¨æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸå…·æœ‰æ·±åšèƒŒæ™¯çš„æ•æ„Ÿæ€§åˆ†æä¸“å®¶ã€‚

**ä»»åŠ¡ï¼š** è¯·ä»”ç»†å®¡æŸ¥å¹¶è§£è¯»ä»¥ä¸‹è¿™ä»½ç”±SALibåº“ç”Ÿæˆçš„**FASTæ•æ„Ÿæ€§åˆ†æ**æŠ¥å‘Šã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1.  **æ€»ç»“æ ¸å¿ƒå‘ç°**ï¼šå¯¹äºæŠ¥å‘Šä¸­æåˆ°çš„æ¯ä¸€ä¸ªè¾“å‡ºæŒ‡æ ‡ï¼ˆå¦‚â€œå¯åŠ¨æ°šé‡â€ç­‰ï¼‰ï¼Œæ€»ç»“å…¶æ•æ„Ÿæ€§åˆ†æç»“æœã€‚
2.  **è¯†åˆ«å…³é”®å‚æ•°**ï¼šæ˜ç¡®æŒ‡å‡ºå“ªäº›è¾“å…¥å‚æ•°çš„**ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆS1ï¼‰**å’Œ**æ€»æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆSTï¼‰**æœ€é«˜ã€‚
3.  **è§£è¯»æŒ‡æ•°å«ä¹‰**ï¼šè§£é‡ŠS1å’ŒSTæŒ‡æ•°çš„å«ä¹‰ã€‚é«˜S1å€¼è¡¨ç¤ºå‚æ•°å¯¹è¾“å‡ºæœ‰é‡è¦çš„ç›´æ¥å½±å“ï¼Œè€ŒSTä¸S1çš„å·®å¼‚è¡¨ç¤ºå‚æ•°å¯èƒ½å­˜åœ¨äº¤äº’ä½œç”¨ã€‚
4.  **æä¾›ç»¼åˆç»“è®º**ï¼šåŸºäºæ‰€æœ‰åˆ†æç»“æœï¼Œå¯¹æ¨¡å‹çš„æ•´ä½“è¡Œä¸ºã€å‚æ•°é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥åŠè¿™äº›å‘ç°å¯¹å·¥ç¨‹å®è·µçš„æ½œåœ¨å¯ç¤ºï¼Œç»™å‡ºä¸€ä¸ªç»¼åˆæ€§çš„ç»“è®ºã€‚

è¯·ç¡®ä¿ä½ çš„åˆ†ææ¸…æ™°ã€ä¸“ä¸šï¼Œå¹¶ç›´æ¥åˆ‡å…¥è¦ç‚¹ã€‚
""",
            "latin": """**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ååœ¨æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸå…·æœ‰æ·±åšèƒŒæ™¯çš„ç»Ÿè®¡å­¦å’Œä¸ç¡®å®šæ€§åˆ†æä¸“å®¶ã€‚

**ä»»åŠ¡ï¼š** è¯·ä»”ç»†å®¡æŸ¥å¹¶è§£è¯»ä»¥ä¸‹è¿™ä»½ç”±æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·ï¼ˆLHSï¼‰ç”Ÿæˆçš„ä¸ç¡®å®šæ€§åˆ†ææŠ¥å‘Šã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1.  **è§£è¯»ç»Ÿè®¡æ•°æ®**ï¼šå¯¹äºæŠ¥å‘Šä¸­çš„æ¯ä¸€ä¸ªè¾“å‡ºæŒ‡æ ‡ï¼ˆå¦‚â€œå¯åŠ¨æ°šé‡â€ç­‰ï¼‰ï¼Œè§£è¯»å…¶å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§/æœ€å°å€¼å’Œç™¾åˆ†ä½æ•°ã€‚
2.  **è¯„ä¼°ä¸ç¡®å®šæ€§**ï¼šåŸºäºæ ‡å‡†å·®å’Œ5%/95%ç™¾åˆ†ä½æ•°çš„èŒƒå›´ï¼Œè¯„ä¼°æ¨¡å‹è¾“å‡ºç»“æœçš„ä¸ç¡®å®šæ€§æˆ–æ³¢åŠ¨èŒƒå›´æœ‰å¤šå¤§ã€‚
3.  **æä¾›ç»¼åˆç»“è®º**ï¼šæ€»ç»“åœ¨ç»™å®šçš„å‚æ•°ä¸ç¡®å®šæ€§ä¸‹ï¼Œæ¨¡å‹çš„å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIsï¼‰è¡¨ç°å¦‚ä½•ï¼Œæ˜¯å¦å­˜åœ¨è¾ƒå¤§çš„é£é™©ï¼ˆä¾‹å¦‚ï¼Œè¾“å‡ºå€¼æ³¢åŠ¨èŒƒå›´è¿‡å¤§ï¼‰ï¼Œå¹¶å¯¹æ¨¡å‹çš„ç¨³å®šæ€§ç»™å‡ºè¯„ä»·ã€‚

è¯·ç¡®ä¿ä½ çš„åˆ†æèšç„¦äºä¸ç¡®å®šæ€§çš„é‡åŒ–å’Œè§£è¯»ï¼Œè€Œä¸æ˜¯å‚æ•°çš„æ•æ„Ÿæ€§æ’åºã€‚
""",
        }

        wrapper_prompt = PROMPT_TEMPLATES.get(
            method,
            """**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ååœ¨æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸå…·æœ‰æ·±åšèƒŒæ™¯çš„æ•æ„Ÿæ€§åˆ†æä¸“å®¶ã€‚

**ä»»åŠ¡ï¼š** è¯·ä»”ç»†å®¡æŸ¥å¹¶è§£è¯»ä»¥ä¸‹è¿™ä»½ç”±SALibåº“ç”Ÿæˆçš„æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Šã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1.  **æ€»ç»“æ ¸å¿ƒå‘ç°**ï¼šç®€æ˜æ‰¼è¦åœ°æ€»ç»“æŠ¥å‘Šä¸­çš„å…³é”®ä¿¡æ¯ã€‚
2.  **è¯†åˆ«å…³é”®å‚æ•°**ï¼šå¯¹äºæŠ¥å‘Šä¸­æåˆ°çš„æ¯ä¸€ä¸ªè¾“å‡ºæŒ‡æ ‡ï¼ˆå¦‚â€œå¯åŠ¨æ°šé‡â€ã€â€œå€å¢æ—¶é—´â€ç­‰ï¼‰ï¼Œæ˜ç¡®æŒ‡å‡ºå“ªäº›è¾“å…¥å‚æ•°å¯¹å®ƒçš„å½±å“æœ€å¤§ï¼ˆå³æœ€æ•æ„Ÿï¼‰ã€‚
3.  **æä¾›ç»¼åˆç»“è®º**ï¼šåŸºäºæ‰€æœ‰åˆ†æç»“æœï¼Œå¯¹æ¨¡å‹çš„æ•´ä½“è¡Œä¸ºã€å‚æ•°é—´çš„ç›¸äº’ä½œç”¨ï¼ˆå¦‚æœå¯èƒ½ï¼‰ä»¥åŠè¿™äº›å‘ç°å¯¹å·¥ç¨‹å®è·µçš„æ½œåœ¨å¯ç¤ºï¼Œç»™å‡ºä¸€ä¸ªç»¼åˆæ€§çš„ç»“è®ºã€‚

è¯·ç¡®ä¿ä½ çš„åˆ†ææ¸…æ™°ã€ä¸“ä¸šï¼Œå¹¶ç›´æ¥åˆ‡å…¥è¦ç‚¹ã€‚
""",
        )

        full_prompt = f"{wrapper_prompt}\n\n---\n**åˆ†ææŠ¥å‘ŠåŸæ–‡ï¼š**\n\n{report_content}"

        max_retries = 3
        for attempt in range(max_retries):
            try:
                client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(
                    f"Sending SALib report to LLM (Attempt {attempt + 1}/{max_retries})..."
                )

                response = client.chat.completions.create(
                    model=ai_model,
                    messages=[{"role": "user", "content": full_prompt}],
                    max_tokens=4000,
                )
                llm_summary = response.choices[0].message.content

                logger.info("LLM analysis successful for SALib report.")
                return wrapper_prompt, llm_summary  # Return wrapper prompt and summary

            except Exception as e:
                logger.error(
                    f"Error calling LLM for SALib report on attempt {attempt + 1}: {e}"
                )
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    logger.error(
                        f"Failed to get LLM summary for SALib report after {max_retries} attempts."
                    )
                    return None, None  # Return None on failure

    except Exception as e:
        logger.error(f"Error in call_llm_for_salib_analysis: {e}", exc_info=True)
        return None, None


def call_llm_for_academic_report(
    analysis_report: str,
    glossary_content: str,
    api_key: str,
    base_url: str,
    ai_model: str,
    problem_details: dict,
    metric_names: list,
    method: str,
    save_dir: str,
) -> Tuple[str, str]:
    """Sends an analysis report and a glossary to an LLM to generate a professional academic report."""
    try:
        logger.info("Proceeding with LLM for academic report generation.")

        param_names_str = ", ".join(
            [f"`{name}`" for name in problem_details.get("names", [])]
        )
        metric_names_str = ", ".join([f"`{name}`" for name in metric_names])

        all_plots = [f for f in os.listdir(save_dir) if f.endswith((".svg", ".png"))]
        plot_list_str = "\n".join([f"    *   `{plot}`" for plot in all_plots])

        method_details = {
            "sobol": {
                "name": "Sobol",
                "methodology": "æŒ‡å‡ºæœ¬æ¬¡åˆ†æé‡‡ç”¨äº†SALibåº“ï¼Œå¹¶ä½¿ç”¨äº†**Sobolæ–¹æ³•**ã€‚è¿™æ˜¯ä¸€ç§åŸºäºæ–¹å·®çš„å…¨å±€æ•æ„Ÿæ€§åˆ†ææŠ€æœ¯ï¼Œèƒ½å¤Ÿé‡åŒ–å•ä¸ªå‚æ•°ä»¥åŠå‚æ•°é—´äº¤äº’ä½œç”¨å¯¹æ¨¡å‹è¾“å‡ºæ–¹å·®çš„è´¡çŒ®ã€‚",
                "results_discussion": """*   å¯¹äºæ¯ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œå“ªäº›è¾“å…¥å‚æ•°çš„ä¸€é˜¶æ•æ„Ÿæ€§ï¼ˆS1ï¼‰å’Œæ€»ä½“æ•æ„Ÿæ€§ï¼ˆSTï¼‰æœ€é«˜ï¼Ÿè¯·ç»“åˆå›¾è¡¨ï¼ˆå¦‚æ¡å½¢å›¾ï¼‰è¿›è¡Œè§£è¯»ã€‚
        *   S1å’ŒSTæŒ‡æ•°ä¹‹é—´çš„å·®å¼‚æ­ç¤ºäº†ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼ŒSTæ˜¾è‘—å¤§äºS1æ„å‘³ç€è¯¥å‚æ•°ä¸å…¶ä»–å‚æ•°å­˜åœ¨æ˜¾è‘—çš„äº¤äº’ä½œç”¨æˆ–å…¶å½±å“æ˜¯éçº¿æ€§çš„ï¼‰ã€‚
        *   åˆ†æä¸åŒæŒ‡æ ‡ä¹‹é—´çš„**æƒè¡¡å…³ç³» (Trade-offs)**ã€‚ä¾‹å¦‚ï¼ŒæŸä¸ªå‚æ•°å¯¹æŸä¸ªæŒ‡æ ‡ (e.g., `Startup_Inventory`) æœ‰æ­£é¢å½±å“ï¼Œä½†å¯èƒ½å¯¹å¦ä¸€ä¸ªæŒ‡æ ‡ (e.g., `Doubling_Time`) æœ‰è´Ÿé¢å½±å“ã€‚""",
            },
            "morris": {
                "name": "Morris",
                "methodology": "æŒ‡å‡ºæœ¬æ¬¡åˆ†æé‡‡ç”¨äº†SALibåº“ï¼Œå¹¶ä½¿ç”¨äº†**Morrisæ–¹æ³•**ã€‚è¿™æ˜¯ä¸€ç§åŸºäºè½¨è¿¹çš„â€œä¸€æ¬¡æ€§â€è®¾è®¡æ–¹æ³•ï¼Œå¸¸ç”¨äºåœ¨é«˜ç»´å‚æ•°ç©ºé—´ä¸­è¿›è¡Œå‚æ•°ç­›é€‰ï¼Œä»¥è¯†åˆ«å‡ºå½±å“æœ€å¤§çš„å°‘æ•°å‡ ä¸ªå‚æ•°ã€‚",
                "results_discussion": """*   å¯¹äºæ¯ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œå“ªäº›å‚æ•°çš„ `Î¼*` (mu_star) å€¼æœ€é«˜ï¼Œè¡¨æ˜å…¶å¯¹è¾“å‡ºçš„æ€»ä½“å½±å“æœ€é‡è¦ï¼Ÿ
        *   `Ïƒ` (sigma) å€¼çš„å¤§å°åˆè¯´æ˜äº†ä»€ä¹ˆï¼Ÿè¾ƒé«˜çš„ `Ïƒ` å€¼é€šå¸¸è¡¨æ˜å‚æ•°å…·æœ‰éçº¿æ€§æ•ˆåº”æˆ–ä¸å…¶ä»–å‚æ•°å­˜åœ¨å¼ºçƒˆçš„äº¤äº’ä½œç”¨ã€‚
        *   è¯·ç»“åˆ `Î¼*-Ïƒ` å›¾è¿›è¡Œåˆ†æï¼Œå¯¹å‚æ•°è¿›è¡Œåˆ†ç±»ï¼ˆä¾‹å¦‚ï¼Œé«˜ `Î¼*`/é«˜ `Ïƒ` vs. é«˜ `Î¼*`/ä½ `Ïƒ`ï¼‰ï¼Œå¹¶è§£é‡Šå…¶å«ä¹‰ã€‚
        *   åˆ†æä¸åŒæŒ‡æ ‡ä¹‹é—´çš„**æƒè¡¡å…³ç³» (Trade-offs)**ã€‚ä¾‹å¦‚ï¼ŒæŸä¸ªå‚æ•°å¯¹æŸä¸ªæŒ‡æ ‡ (e.g., `Startup_Inventory`) æœ‰æ­£é¢å½±å“ï¼Œä½†å¯èƒ½å¯¹å¦ä¸€ä¸ªæŒ‡æ ‡ (e.g., `Doubling_Time`) æœ‰è´Ÿé¢å½±å“ã€‚""",
            },
            "fast": {
                "name": "FAST",
                "methodology": "æŒ‡å‡ºæœ¬æ¬¡åˆ†æé‡‡ç”¨äº†SALibåº“ï¼Œå¹¶ä½¿ç”¨äº†**FASTï¼ˆå‚…é‡Œå¶å¹…åº¦æ•æ„Ÿæ€§æ£€éªŒï¼‰æ–¹æ³•**ã€‚è¿™æ˜¯ä¸€ç§åŸºäºé¢‘ç‡çš„å…¨å±€æ•æ„Ÿæ€§åˆ†ææŠ€æœ¯ï¼Œé€šè¿‡å°†å‚æ•°åœ¨å‚…é‡Œå¶çº§æ•°ä¸­å±•å¼€æ¥è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ•°ã€‚",
                "results_discussion": """*   å¯¹äºæ¯ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œå“ªäº›è¾“å…¥å‚æ•°çš„ä¸€é˜¶æ•æ„Ÿæ€§ï¼ˆS1ï¼‰æœ€é«˜ï¼Ÿ
        *   ï¼ˆå¦‚æœå¯ç”¨ï¼‰æ€»ä½“æ•æ„Ÿæ€§ï¼ˆSTï¼‰ä¸ä¸€é˜¶æ•æ„Ÿæ€§ï¼ˆS1ï¼‰çš„æ¯”è¾ƒæ­ç¤ºäº†ä»€ä¹ˆï¼Ÿè¾ƒå¤§çš„å·®å¼‚é€šå¸¸è¡¨æ˜å­˜åœ¨å‚æ•°äº¤äº’ã€‚
        *   åˆ†æä¸åŒæŒ‡æ ‡ä¹‹é—´çš„**æƒè¡¡å…³ç³» (Trade-offs)**ã€‚ä¾‹å¦‚ï¼ŒæŸä¸ªå‚æ•°å¯¹æŸä¸ªæŒ‡æ ‡ (e.g., `Startup_Inventory`) æœ‰æ­£é¢å½±å“ï¼Œä½†å¯èƒ½å¯¹å¦ä¸€ä¸ªæŒ‡æ ‡ (e.g., `Doubling_Time`) æœ‰è´Ÿé¢å½±å“ã€‚""",
            },
        }

        if method == "latin":
            ACADEMIC_REPORT_PROMPT_WRAPPER = f"""**è§’è‰²ï¼š** æ‚¨æ˜¯ä¸€ä½åœ¨æ ¸èšå˜å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸï¼Œå…·æœ‰æ·±åšå­¦æœ¯èƒŒæ™¯çš„èµ„æ·±ç§‘å­¦å®¶ï¼Œæ“…é•¿è¿›è¡Œ**ä¸ç¡®å®šæ€§é‡åŒ– (UQ)** å’Œé£é™©è¯„ä¼°ã€‚

**ä»»åŠ¡ï¼š** æ‚¨æ”¶åˆ°äº†ä¸€ä¸ªåŸºäº**æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ · (LHS)** çš„ä¸ç¡®å®šæ€§åˆ†æåˆæ­¥æŠ¥å‘Šå’Œä¸€ä»½ä¸“ä¸šæœ¯è¯­è¡¨ã€‚è¯·æ‚¨åŸºäºè¿™ä¸¤ä»½æ–‡ä»¶ï¼Œæ’°å†™ä¸€ä»½æ›´åŠ ä¸“ä¸šã€æ­£å¼ã€ç¬¦åˆå­¦æœ¯å‘è¡¨æ ‡å‡†çš„æ·±åº¦åˆ†ææ€»ç»“æŠ¥å‘Šã€‚

**æŒ‡ä»¤ï¼š**

1.  **ä¸“ä¸šåŒ–è¯­è¨€ï¼š** å°†åˆæ­¥æŠ¥å‘Šä¸­çš„æ¨¡å‹å‚æ•°/ç¼©å†™æ›¿æ¢ä¸ºæœ¯è¯­è¡¨ä¸­å¯¹åº”çš„ä¸“ä¸šè¯æ±‡ã€‚
2.  **å­¦æœ¯åŒ–é‡è¿°ï¼š** ç”¨ä¸¥è°¨ã€å®¢è§‚çš„å­¦æœ¯è¯­è¨€é‡æ–°ç»„ç»‡å’Œé˜è¿°åˆæ­¥æŠ¥å‘Šä¸­çš„å‘ç°ï¼Œèšç„¦äº**ä¸ç¡®å®šæ€§**çš„é‡åŒ–å’Œè§£è¯»ã€‚
3.  **å›¾è¡¨å’Œè¡¨æ ¼çš„å‘ˆç°ä¸å¼•ç”¨ï¼š**
    *   **æ˜¾ç¤ºå›¾è¡¨ï¼š** åœ¨æŠ¥å‘Šçš„â€œç»“æœä¸è®¨è®ºâ€éƒ¨åˆ†ï¼Œæ‚¨**å¿…é¡»**ä½¿ç”¨Markdownè¯­æ³• `![å›¾è¡¨æ ‡é¢˜](å›¾è¡¨æ–‡ä»¶å)` æ¥**ç›´æ¥åµŒå…¥**å’Œæ˜¾ç¤ºåˆæ­¥æŠ¥å‘Šä¸­åŒ…å«çš„æ‰€æœ‰å›¾è¡¨ã€‚å¯ç”¨çš„å›¾è¡¨æ–‡ä»¶å¦‚ä¸‹ï¼š
{plot_list_str}
    *   **å¼•ç”¨å›¾è¡¨ï¼š** åœ¨æ­£æ–‡ä¸­åˆ†æå’Œè®¨è®ºå›¾è¡¨å†…å®¹æ—¶ï¼Œè¯·ä½¿ç”¨â€œå¦‚å›¾1æ‰€ç¤º...â€ç­‰æ–¹å¼å¯¹å›¾è¡¨è¿›è¡Œç¼–å·å’Œæ–‡å­—å¼•ç”¨ã€‚
    *   **æ˜¾ç¤ºè¡¨æ ¼ï¼š** å½“å‘ˆç°æ•°æ®æ—¶ï¼ˆä¾‹å¦‚ï¼Œç»Ÿè®¡æ‘˜è¦ã€åˆ†å¸ƒæ•°æ®ç­‰ï¼‰ï¼Œæ‚¨**å¿…é¡»**ä½¿ç”¨Markdownçš„ç®¡é“è¡¨æ ¼ï¼ˆpipe-tableï¼‰æ ¼å¼æ¥æ¸…æ™°åœ°å±•ç¤ºå®ƒä»¬ã€‚æ‚¨å¯ä»¥ç›´æ¥å¤ç”¨æˆ–é‡æ–°æ ¼å¼åŒ–åˆæ­¥æŠ¥å‘Šä¸­çš„æ•°æ®è¡¨æ ¼ã€‚
4.  **ç»“æ„åŒ–æŠ¥å‘Šï¼š** æ‚¨çš„æŠ¥å‘Šæ˜¯å…³äºä¸€é¡¹**ä¸ç¡®å®šæ€§åˆ†æ**ã€‚æŠ¥å‘Šåº”åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
    *   **æ‘˜è¦ (Abstract):** ç®€è¦æ¦‚æ‹¬æœ¬æ¬¡ä¸ç¡®å®šæ€§ç ”ç©¶çš„ç›®çš„ï¼Œæ˜ç¡®æŒ‡å‡ºåˆ†æçš„è¾“å…¥å‚æ•°æ˜¯ {param_names_str}ï¼Œæ€»ç»“è¿™äº›å‚æ•°çš„ä¸ç¡®å®šæ€§å¯¹å…³é”®æ€§èƒ½æŒ‡æ ‡ ({metric_names_str}) çš„è¾“å‡ºåˆ†å¸ƒï¼ˆå¦‚å‡å€¼ã€æ ‡å‡†å·®ã€ç½®ä¿¡åŒºé—´ï¼‰æœ‰ä½•å½±å“ã€‚
    *   **å¼•è¨€ (Introduction):** æè¿°è¿›è¡Œè¿™é¡¹ä¸ç¡®å®šæ€§åˆ†æçš„èƒŒæ™¯å’Œé‡è¦æ€§ã€‚é˜è¿°ç ”ç©¶ç›®æ ‡ï¼Œå³é‡åŒ–è¯„ä¼°å½“è¾“å…¥å‚æ•° {param_names_str} åœ¨å…¶å®šä¹‰åŸŸå†…å˜åŒ–æ—¶ï¼Œæ°šç‡ƒæ–™å¾ªç¯ç³»ç»Ÿå…³é”®æ€§èƒ½æŒ‡æ ‡çš„ç»Ÿè®¡åˆ†å¸ƒå’Œç¨³å®šæ€§ã€‚
    *   **æ–¹æ³• (Methodology):** ç®€è¦è¯´æ˜åˆ†ææ–¹æ³•ã€‚æŒ‡å‡ºæœ¬æ¬¡åˆ†æé‡‡ç”¨äº†æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·ï¼ˆLHSï¼‰æ–¹æ³•æ¥å¯¹è¾“å…¥å‚æ•°ç©ºé—´è¿›è¡ŒæŠ½æ ·ã€‚è¯´æ˜è¢«è¯„ä¼°çš„å…³é”®æ€§èƒ½æŒ‡æ ‡æ˜¯ {metric_names_str}ï¼Œä»¥åŠè¾“å…¥å‚æ•°çš„æ¦‚ç‡åˆ†å¸ƒå’ŒèŒƒå›´ã€‚
    *   **ç»“æœä¸è®¨è®º (Results and Discussion):** è¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒã€‚è¯·ç»“åˆåˆæ­¥æŠ¥å‘Šä¸­çš„ç»Ÿè®¡æ•°æ®å’Œæ‚¨åµŒå…¥çš„å›¾è¡¨ï¼ˆå¦‚ç›´æ–¹å›¾ã€ç´¯ç§¯åˆ†å¸ƒå›¾ï¼‰ï¼Œåˆ†ç‚¹è¯¦ç»†è®ºè¿°ï¼š
        *   å¯¹äºæ¯ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œå…¶è¾“å‡ºçš„**æ¦‚ç‡åˆ†å¸ƒ**æ˜¯æ€æ ·çš„ï¼Ÿï¼ˆä¾‹å¦‚ï¼Œæ˜¯æ­£æ€åˆ†å¸ƒã€åæ€åˆ†å¸ƒè¿˜æ˜¯åŒå³°åˆ†å¸ƒï¼Ÿï¼‰
        *   è¾“å‡ºæŒ‡æ ‡çš„**ä¸ç¡®å®šæ€§èŒƒå›´**æœ‰å¤šå¤§ï¼Ÿï¼ˆå‚è€ƒæ ‡å‡†å·®å’Œ5%-95%ç™¾åˆ†ä½æ•°åŒºé—´ï¼‰ã€‚è¿™ä¸ªèŒƒå›´åœ¨å·¥ç¨‹å®è·µä¸­æ˜¯å¦å¯ä»¥æ¥å—ï¼Ÿ
        *   æ˜¯å¦å­˜åœ¨æŸäº›æŒ‡æ ‡çš„æ³¢åŠ¨èŒƒå›´è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿæ€§èƒ½ä½äºè®¾è®¡è¦æ±‚æˆ–å­˜åœ¨è¿è¡Œé£é™©ï¼Ÿ
    *   **ç»“è®º (Conclusion):** æ€»ç»“æœ¬æ¬¡ä¸ç¡®å®šæ€§åˆ†æå¾—å‡ºçš„ä¸»è¦å­¦æœ¯ç»“è®ºï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹çš„ç¨³å®šæ€§ã€è¾“å‡ºæŒ‡æ ‡çš„å¯é æ€§ç­‰ï¼‰ï¼Œå¹¶å¯¹é™ä½å…³é”®æŒ‡æ ‡ä¸ç¡®å®šæ€§æˆ–æœªæ¥çš„é£é™©è¯„ä¼°æå‡ºå…·ä½“å»ºè®®ã€‚
5.  **è¾“å‡ºæ ¼å¼ï¼š** è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„å­¦æœ¯åˆ†ææŠ¥å‘Šæ­£æ–‡ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½éµå¾ªæ­£ç¡®çš„Markdownè¯­æ³•ã€‚

**è¾“å…¥æ–‡ä»¶ï¼š**
"""
        else:
            selected_method = method_details.get(method)
            if not selected_method:
                # Fallback for unknown methods
                selected_method = {
                    "name": method.capitalize(),
                    "methodology": f"æŒ‡å‡ºæœ¬æ¬¡åˆ†æé‡‡ç”¨äº†SALibåº“ï¼Œå¹¶æåŠå…·ä½“çš„æ•æ„Ÿæ€§åˆ†ææ–¹æ³•ä¸º**{method.capitalize()}**ã€‚",
                    "results_discussion": "*   å¯¹äºæ¯ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œè¯†åˆ«å‡ºæœ€é‡è¦çš„è¾“å…¥å‚æ•°ã€‚\n*   è®¨è®ºè¿™äº›å‘ç°çš„æ„ä¹‰ã€‚",
                }

            ACADEMIC_REPORT_PROMPT_WRAPPER = f"""**è§’è‰²ï¼š** æ‚¨æ˜¯ä¸€ä½åœ¨æ ¸èšå˜å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯æ°šç‡ƒæ–™å¾ªç¯é¢†åŸŸï¼Œå…·æœ‰æ·±åšå­¦æœ¯èƒŒæ™¯çš„èµ„æ·±ç§‘å­¦å®¶ã€‚

**ä»»åŠ¡ï¼š** æ‚¨æ”¶åˆ°äº†ä¸€ä¸ªå…³äº**SALib {selected_method['name']} æ–¹æ³•æ•æ„Ÿæ€§åˆ†æ**çš„ç¨‹åºç”Ÿæˆçš„åˆæ­¥æŠ¥å‘Šå’Œä¸€ä»½ä¸“ä¸šæœ¯è¯­è¡¨ã€‚è¯·æ‚¨åŸºäºè¿™ä¸¤ä»½æ–‡ä»¶ï¼Œæ’°å†™ä¸€ä»½æ›´åŠ ä¸“ä¸šã€æ­£å¼ã€ç¬¦åˆå­¦æœ¯å‘è¡¨æ ‡å‡†çš„æ·±åº¦åˆ†ææ€»ç»“æŠ¥å‘Šã€‚

**æŒ‡ä»¤ï¼š**

1.  **ä¸“ä¸šåŒ–è¯­è¨€ï¼š** å°†åˆæ­¥æŠ¥å‘Šä¸­çš„æ¨¡å‹å‚æ•°/ç¼©å†™ï¼ˆä¾‹å¦‚ `sds.I[1]`, `Startup_Inventory`ï¼‰æ›¿æ¢ä¸ºæœ¯è¯­è¡¨ä¸­å¯¹åº”çš„â€œä¸­æ–‡ç¿»è¯‘â€æˆ–â€œè‹±æ–‡æœ¯è¯­â€ã€‚
2.  **å­¦æœ¯åŒ–é‡è¿°ï¼š** ç”¨ä¸¥è°¨ã€å®¢è§‚çš„å­¦æœ¯è¯­è¨€é‡æ–°ç»„ç»‡å’Œé˜è¿°åˆæ­¥æŠ¥å‘Šä¸­çš„å‘ç°ã€‚
3.  **å›¾è¡¨å’Œè¡¨æ ¼çš„å‘ˆç°ä¸å¼•ç”¨ï¼š**
    *   **æ˜¾ç¤ºå›¾è¡¨ï¼š** åœ¨æŠ¥å‘Šçš„â€œç»“æœä¸è®¨è®ºâ€éƒ¨åˆ†ï¼Œæ‚¨**å¿…é¡»**ä½¿ç”¨Markdownè¯­æ³• `![å›¾è¡¨æ ‡é¢˜](å›¾è¡¨æ–‡ä»¶å)` æ¥**ç›´æ¥åµŒå…¥**å’Œæ˜¾ç¤ºåˆæ­¥æŠ¥å‘Šä¸­åŒ…å«çš„æ‰€æœ‰å›¾è¡¨ã€‚å¯ç”¨çš„å›¾è¡¨æ–‡ä»¶å¦‚ä¸‹ï¼š
{plot_list_str}
    *   **å¼•ç”¨å›¾è¡¨ï¼š** åœ¨æ­£æ–‡ä¸­åˆ†æå’Œè®¨è®ºå›¾è¡¨å†…å®¹æ—¶ï¼Œè¯·ä½¿ç”¨â€œå¦‚å›¾1æ‰€ç¤º...â€ç­‰æ–¹å¼å¯¹å›¾è¡¨è¿›è¡Œç¼–å·å’Œæ–‡å­—å¼•ç”¨ã€‚
    *   **æ˜¾ç¤ºè¡¨æ ¼ï¼š** å½“å‘ˆç°æ•°æ®æ—¶ï¼ˆä¾‹å¦‚ï¼Œæ•æ„Ÿæ€§æŒ‡æ•°è¡¨ï¼‰ï¼Œæ‚¨**å¿…é¡»**ä½¿ç”¨Markdownçš„ç®¡é“è¡¨æ ¼ï¼ˆpipe-tableï¼‰æ ¼å¼æ¥æ¸…æ™°åœ°å±•ç¤ºå®ƒä»¬ã€‚æ‚¨å¯ä»¥ç›´æ¥å¤ç”¨æˆ–é‡æ–°æ ¼å¼åŒ–åˆæ­¥æŠ¥å‘Šä¸­çš„æ•°æ®è¡¨æ ¼ã€‚
4.  **ç»“æ„åŒ–æŠ¥å‘Šï¼š** æ‚¨çš„æŠ¥å‘Šæ˜¯å…³äºä¸€é¡¹**æ•æ„Ÿæ€§åˆ†æ**ã€‚æŠ¥å‘Šåº”åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
    *   **æ‘˜è¦ (Abstract):** ç®€è¦æ¦‚æ‹¬æœ¬æ¬¡æ•æ„Ÿæ€§ç ”ç©¶çš„ç›®çš„ï¼Œæ˜ç¡®æŒ‡æ˜åˆ†æçš„è¾“å…¥å‚æ•°æ˜¯ {param_names_str}ï¼Œæ€»ç»“å“ªäº›å‚æ•°å¯¹å…³é”®æ€§èƒ½æŒ‡æ ‡ ({metric_names_str}) å½±å“æœ€æ˜¾è‘—ï¼Œå¹¶é™ˆè¿°æ ¸å¿ƒç»“è®ºã€‚
    *   **å¼•è¨€ (Introduction):** æè¿°è¿›è¡Œè¿™é¡¹æ•æ„Ÿæ€§åˆ†æçš„èƒŒæ™¯å’Œé‡è¦æ€§ã€‚é˜è¿°ç ”ç©¶ç›®æ ‡ï¼Œå³é‡åŒ–è¯„ä¼°è¾“å…¥å‚æ•°çš„å˜åŒ–å¯¹æ°šç‡ƒæ–™å¾ªç¯ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚
    *   **æ–¹æ³• (Methodology):** {selected_method['methodology']} è¯´æ˜è¢«è¯„ä¼°çš„å…³é”®æ€§èƒ½æŒ‡æ ‡æ˜¯ {metric_names_str}ï¼Œä»¥åŠè¾“å…¥å‚æ•° {param_names_str} çš„å˜åŒ–èŒƒå›´ã€‚
    *   **ç»“æœä¸è®¨è®º (Results and Discussion):** è¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒã€‚è¯·ç»“åˆåˆæ­¥æŠ¥å‘Šä¸­çš„æ•°æ®å’Œæ‚¨åµŒå…¥çš„å›¾è¡¨ï¼Œåˆ†ç‚¹è¯¦ç»†è®ºè¿°ï¼š
{selected_method['results_discussion']}
    *   **ç»“è®º (Conclusion):** æ€»ç»“æœ¬æ¬¡æ•æ„Ÿæ€§åˆ†æå¾—å‡ºçš„ä¸»è¦å­¦æœ¯ç»“è®ºï¼Œå¹¶å¯¹ååº”å †è®¾è®¡æˆ–æœªæ¥ç ”ç©¶æ–¹å‘æå‡ºå…·ä½“å»ºè®®ã€‚
5.  **è¾“å‡ºæ ¼å¼ï¼š** è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„å­¦æœ¯åˆ†ææŠ¥å‘Šæ­£æ–‡ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½éµå¾ªæ­£ç¡®çš„Markdownè¯­æ³•ã€‚

**è¾“å…¥æ–‡ä»¶ï¼š**
"""

        full_prompt = f"{ACADEMIC_REPORT_PROMPT_WRAPPER}\n\n---\n### 1. åˆæ­¥åˆ†ææŠ¥å‘Š\n---\n{analysis_report}\n\n---\n### 2. ä¸“ä¸šæœ¯è¯­è¡¨\n---\n{glossary_content}"

        max_retries = 3
        for attempt in range(max_retries):
            try:
                client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(
                    f"Sending data for academic report to LLM (Attempt {attempt + 1}/{max_retries})..."
                )

                response = client.chat.completions.create(
                    model=ai_model,
                    messages=[{"role": "user", "content": full_prompt}],
                    max_tokens=4000,
                )
                academic_report = response.choices[0].message.content

                logger.info("LLM academic report generation successful.")
                return ACADEMIC_REPORT_PROMPT_WRAPPER, academic_report

            except Exception as e:
                logger.error(
                    f"Error calling LLM for academic report on attempt {attempt + 1}: {e}"
                )
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    logger.error(
                        f"Failed to get LLM academic report after {max_retries} attempts."
                    )
                    return None, None

    except Exception as e:
        logger.error(f"Error in call_llm_for_academic_report: {e}", exc_info=True)
        return None, None


def run_salib_analysis(config: Dict[str, Any]) -> None:
    """
    Orchestrates the SALib sensitivity analysis workflow.

    This function extracts the necessary configuration, defines the problem
    space for SALib, and then runs the analysis.

    Args:
        config: The main configuration dictionary.
    """
    # 1. Extract sensitivity analysis configuration
    sa_config = config.get("sensitivity_analysis")
    if not sa_config or not sa_config.get("enabled"):
        logger.info("Sensitivity analysis is not enabled in the configuration file.")
        return

    # 2. Create analyzer
    analyzer = TricysSALibAnalyzer(config)

    # 3. Define the problem space from configuration
    analysis_case = sa_config.get("analysis_case", {})
    param_names = analysis_case.get("independent_variable")
    sampling_details = analysis_case.get("independent_variable_sampling")

    if not isinstance(param_names, list):
        raise ValueError("'independent_variable' must be a list of parameter names.")
    if not isinstance(sampling_details, dict):
        raise ValueError(
            "'independent_variable_sampling' must be an object with parameter details."
        )

    param_bounds = {
        name: sampling_details[name]["bounds"]
        for name in param_names
        if name in sampling_details
    }
    param_dists = {
        name: sampling_details[name].get("distribution", "unif")
        for name in param_names
        if name in sampling_details
    }

    if len(param_bounds) != len(param_names):
        raise ValueError(
            "The keys of 'independent_variable' and 'independent_variable_sampling' do not match"
        )

    problem = analyzer.define_problem(param_bounds, param_dists)
    logger.info(
        f"\nğŸ” The problem space with {problem['num_vars']} parameters was defined from the configuration file"
    )

    # 4. Generate samples from configuration
    analyzer_config = analysis_case.get("analyzer", {})
    enabled_method_name = analyzer_config.get("method")
    if not enabled_method_name:
        raise ValueError(
            "No method found in 'sensitivity_analysis.analysis_case.analyzer'"
        )

    N = analyzer_config.get("sample_N", 1024)

    sample_kwargs = {}

    samples = analyzer.generate_samples(
        method=enabled_method_name, N=N, **sample_kwargs
    )
    logger.info(f"âœ“ Generated {len(samples)} parameter samples")

    # 5. Run Tricys simulation
    output_metrics = analysis_case.get("dependent_variables", [])

    csv_file_path = analyzer.run_tricys_simulations(output_metrics=output_metrics)
    logger.info(f"âœ“ Parameter file has been generated: {csv_file_path}")

    summary_file = None
    try:
        logger.info("\nAttempting to run Tricys analysis directly...")
        summary_file = analyzer.run_tricys_analysis(
            csv_file_path=csv_file_path, output_metrics=output_metrics
        )
        if summary_file:
            logger.info(f"âœ“ Tricys analysis completed, result file: {summary_file}")
        else:
            logger.info("âš ï¸  Tricys analysis result file not found")
            return
    except Exception as e:
        logger.info(f"âš ï¸  Tricys analysis failed: {e}")
        logger.info("Please check if the model path and configuration are correct")
        return

    # 6. Run SALib analysis from Tricys results
    try:
        logger.info("\nRunning SALib analysis from Tricys results...")
        all_results = analyzer.run_salib_analysis_from_tricys_results(
            sensitivity_summary_csv=summary_file,
            param_bounds=param_bounds,
            output_metrics=output_metrics,
            methods=[enabled_method_name],
            save_dir=os.path.dirname(summary_file),
        )

        logger.info(f"\nâœ… SALib {enabled_method_name.upper()} analysis completed!")
        logger.info(
            f"ğŸ“ The results have been saved to: {os.path.join(os.path.dirname(summary_file), f'salib_analysis_{enabled_method_name}')}"
        )

        logger.info("\nğŸ“ˆ Brief results:")
        for metric_name, metric_results in all_results.items():
            logger.info(f"\n--- {metric_name} ---")
            if enabled_method_name in metric_results:
                result_data = metric_results[enabled_method_name]
                if enabled_method_name == "sobol":
                    logger.info("ğŸ”¥ Most sensitive parameters (Sobol ST):")
                    st_values = list(zip(analyzer.problem["names"], result_data["ST"]))
                    st_values.sort(key=lambda x: x[1], reverse=True)
                    for param, st in st_values[:3]:
                        logger.info(f"   {param}: {st:.4f}")
                elif enabled_method_name == "morris":
                    logger.info("ğŸ“Š Most Sensitive Parameter (Morris Î¼*):")
                    mu_star_values = list(
                        zip(analyzer.problem["names"], result_data["mu_star"])
                    )
                    mu_star_values.sort(key=lambda x: x[1], reverse=True)
                    for param, mu_star in mu_star_values[:3]:
                        logger.info(f"   {param}: {mu_star:.4f}")
                elif enabled_method_name == "fast":
                    logger.info("âš¡ Most Sensitive Parameter (Morris Î¼*):")
                    st_values = list(zip(analyzer.problem["names"], result_data["ST"]))
                    st_values.sort(key=lambda x: x[1], reverse=True)
                    for param, st in st_values[:3]:
                        logger.info(f"   {param}: {st:.4f}")

        return analyzer, all_results

    except Exception as e:
        logger.error(f"SALib analysis failed: {e}", exc_info=True)
        raise
